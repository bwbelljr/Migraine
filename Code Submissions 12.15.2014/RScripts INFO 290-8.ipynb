{
 "metadata": {
  "name": "",
  "signature": "sha256:34078321ab092614bbe2e37fde949ee99fabbffbc04ebe627bbb9c91fc6de25c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "multiPIM.R: This script performs variable importance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Install and load multiPIM package\n",
      "\n",
      "install.packages(\"multiPIM\")\n",
      "\n",
      "# Code for 24 hour exposures\n",
      "\n",
      "# Load data from migraine_24hr_only.csv into dataframe ObsData\n",
      "ObsData<- read.csv(\"migraine_24hr_only.csv\")\n",
      "\n",
      "# Generate data frame containing binary exposure variables\n",
      "A <- subset(ObsData, select=-c(time_24hr, time, headache_24hr))\n",
      "\n",
      "# Generate data frame containing continuous outcome variable Y\n",
      "Y <- subset(ObsData, select=headache_24hr)\n",
      "\n",
      "# Call multiPIM method\n",
      "#\n",
      "# Specify Y, A, and optional W data frames. \n",
      "# estimator = \"TMLE\" is targeted maximum likelihood\n",
      "# g.method=\"sl\" specifies super learning for g (treatment mechanism)\n",
      "# g.sl.cands = all.bin.cands means that we create a library of candidate algorithms for fitting g, consisting of polyclass (polspline), penalized bin (L1 penalized logistic regression), main terms logistic and rpart.bin (recursive partitioning trees for binary outcomes).\n",
      "# g.num.folds=10, g.num.splits=1 means we do 10-fold cross validation with 1 split for g super learning.\n",
      "# Q.method=\"sl\" specifies super learning for Q (outcome regression)\n",
      "# Q.sl.cands=\"all\" means that we create a library of candidate algorithms for fitting Q, consisting of polymars (multivariate adaptive regression splines), lars (lars package), main terms linear regression, L1 penalized linear regression (LASSO), and rpart.cont (recursive partitioning trees for continuous outcomes).\n",
      "# Q.num.folds=10, Q.num.splits=1 means we do 10-fold cross validation with 1 split for Q super learning.\n",
      "# Q.type=\"continuous.outcome\" specifies that outcome variable is continuous\n",
      "# adjust.for.other.As=TRUE means that each column of A (covariates/features) is included in fit of g (treatment mechanism) and Q (outcome regression)\n",
      "# For future reference, use extra.cands argument to add additional machine learning algorihtms for fitting g and Q\n",
      "\n",
      "result <- multiPIM(Y, A, W=NULL, estimator = \"TMLE\", g.method=\"sl\", g.sl.cands = all.bin.cands, g.num.folds=10, g.num.splits=1, Q.method=\"sl\", Q.sl.cands=\"all\", Q.num.folds=10, Q.num.splits=1, Q.type=\"continuous.outcome\", adjust.for.other.As=TRUE)\n",
      "\n",
      "#####################################################################################\n",
      "# Code for 8 hour exposures\n",
      "\n",
      "# Load data from migraine_24hr_only.csv into dataframe ObsData\n",
      "ObsData_8hr <- read.csv(\"migraine_8hr_only.csv\")\n",
      "\n",
      "# Generate data frame containing binary exposure variables\n",
      "A_8hr <- subset(ObsData_8hr, select=-c(time_8hr, time, headache_8hr))\n",
      "\n",
      "# Generate data frame containing continuous outcome variable Y\n",
      "Y_8hr <- subset(ObsData_8hr, select=headache_8hr)\n",
      "\n",
      "# Call multiPIM method\n",
      "#\n",
      "# Specify Y, A, and optional W data frames. \n",
      "# estimator = \"TMLE\" is targeted maximum likelihood\n",
      "# g.method=\"sl\" specifies super learning for g (treatment mechanism)\n",
      "# g.sl.cands = all.bin.cands means that we create a library of candidate algorithms for fitting g, consisting of polyclass (polspline), penalized bin (L1 penalized logistic regression), main terms logistic and rpart.bin (recursive partitioning trees for binary outcomes).\n",
      "# g.num.folds=10, g.num.splits=1 means we do 10-fold cross validation with 1 split for g super learning.\n",
      "# Q.method=\"sl\" specifies super learning for Q (outcome regression)\n",
      "# Q.sl.cands=\"all\" means that we create a library of candidate algorithms for fitting Q, consisting of polymars (multivariate adaptive regression splines), lars (lars package), main terms linear regression, L1 penalized linear regression (LASSO), and rpart.cont (recursive partitioning trees for continuous outcomes).\n",
      "# Q.num.folds=10, Q.num.splits=1 means we do 10-fold cross validation with 1 split for Q super learning.\n",
      "# Q.type=\"continuous.outcome\" specifies that outcome variable is continuous\n",
      "# adjust.for.other.As=TRUE means that each column of A (covariates/features) is included in fit of g (treatment mechanism) and Q (outcome regression)\n",
      "# For future reference, use extra.cands argument to add additional machine learning algorihtms for fitting g and Q\n",
      "\n",
      "result_8hr <- multiPIM(Y_8hr, A_8hr, W=NULL, estimator = \"TMLE\", g.method=\"sl\", g.sl.cands = all.bin.cands, g.num.folds=10, g.num.splits=1, Q.method=\"sl\", Q.sl.cands=\"all\", Q.num.folds=10, Q.num.splits=1, Q.type=\"continuous.outcome\", adjust.for.other.As=TRUE)\n",
      "\n",
      "\n",
      "#####################################################################################\n",
      "# Code for 2 hour exposures\n",
      "\n",
      "# Load data from migraine_24hr_only.csv into dataframe ObsData\n",
      "ObsData_2hr <- read.csv(\"migraine_2hr_only.csv\")\n",
      "\n",
      "# Generate data frame containing binary exposure variables\n",
      "A_2hr <- subset(ObsData_2hr, select=-c(time_2hr, time, headache_2hr))\n",
      "\n",
      "# Generate data frame containing continuous outcome variable Y\n",
      "Y_2hr <- subset(ObsData_2hr, select=headache_2hr)\n",
      "\n",
      "# Call multiPIM method\n",
      "#\n",
      "# Specify Y, A, and optional W data frames. \n",
      "# estimator = \"TMLE\" is targeted maximum likelihood\n",
      "# g.method=\"sl\" specifies super learning for g (treatment mechanism)\n",
      "# g.sl.cands = all.bin.cands means that we create a library of candidate algorithms for fitting g, consisting of polyclass (polspline), penalized bin (L1 penalized logistic regression), main terms logistic and rpart.bin (recursive partitioning trees for binary outcomes).\n",
      "# g.num.folds=10, g.num.splits=1 means we do 10-fold cross validation with 1 split for g super learning.\n",
      "# Q.method=\"sl\" specifies super learning for Q (outcome regression)\n",
      "# Q.sl.cands=\"all\" means that we create a library of candidate algorithms for fitting Q, consisting of polymars (multivariate adaptive regression splines), lars (lars package), main terms linear regression, L1 penalized linear regression (LASSO), and rpart.cont (recursive partitioning trees for continuous outcomes).\n",
      "# Q.num.folds=10, Q.num.splits=1 means we do 10-fold cross validation with 1 split for Q super learning.\n",
      "# Q.type=\"continuous.outcome\" specifies that outcome variable is continuous\n",
      "# adjust.for.other.As=TRUE means that each column of A (covariates/features) is included in fit of g (treatment mechanism) and Q (outcome regression)\n",
      "# For future reference, use extra.cands argument to add additional machine learning algorihtms for fitting g and Q\n",
      "\n",
      "result_2hr <- multiPIM(Y_2hr, A_2hr, W=NULL, estimator = \"TMLE\", g.method=\"sl\", g.sl.cands = all.bin.cands, g.num.folds=10, g.num.splits=1, Q.method=\"sl\", Q.sl.cands=\"all\", Q.num.folds=10, Q.num.splits=1, Q.type=\"continuous.outcome\", adjust.for.other.As=TRUE)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "forecast_TBATS.R: This script applies time series forecasting model (TBATS) to full data (March 6-Dec 1, 2014) for prediction of migraine level Dec 2-3, 2014"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add this to console window for sense.io\n",
      "\n",
      "# Set number of cores to 32...\n",
      "\n",
      "library('sense')\n",
      "\n",
      "# optionally include ggplot commands...\n",
      "\n",
      "library(\"parallel\")\n",
      "detectCores() # This should be 32\n",
      "numCores = detectCores()\n",
      "numCores\n",
      "\n",
      "# install.packages(\"forecast\")\n",
      "library(\"forecast\")\n",
      "\n",
      "#mydata = read.csv(\"Headache.csv\")\n",
      "mydata = read.csv(\"Headache_Aug_16_23.csv\")\n",
      "\n",
      "# Extract number of rows in data frame\n",
      "mydata_length <- nrow(mydata)\n",
      "mydata_length\n",
      "\n",
      "# Extract latest time\n",
      "mdata_last_time <- mydata$time[mydata_length]\n",
      "mdata_last_time\n",
      "\n",
      "plot(mydata$headache)\n",
      "\n",
      "# Create msts() object for headache...\n",
      "# This means that I can have multiple frequencies: hour (60) and day (1440)\n",
      "ha_msts_many <- msts(mydata$headache, seasonal.periods=c(30, 60, 120, 180, 240, 360, 480, 720, 1440))\n",
      "\n",
      "# ha_msts_many <- msts(mydata$headache, seasonal.periods=c(12, 30, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200, 1260, 1320, 1380, 1440))\n",
      "\n",
      "# Fit TBATS on data with multiple seasons\n",
      " \n",
      "ha_tbats_many_fit <- tbats(ha_msts_many, h= 1440, use.parallel=TRUE, num.cores=numCores)\n",
      "\n",
      "# Next time, time this code!!! My estimate is approximately 2 hours on 32 cores...\n",
      "\n",
      "# Create forecast object\n",
      "ha_tbats_many_forecast <- forecast(ha_tbats_many_fit) \n",
      "\n",
      "# plot simple forecast\n",
      "plot(ha_tbats_many_forecast)\n",
      "\n",
      "# A list containing information about the fitted model\n",
      "ha_tbats_many_forecast$model\n",
      "\n",
      "# TBATS(1, {1,0}, 0.997, {<30,4>, <60,1>, <120,1>, <180,1>, <240,1>, <360,1>, <480,1>, <720,1>, <1440,1>})\n",
      "\n",
      "#Parameters\n",
      "#Parameters\n",
      "#  Alpha: 0.01091734\n",
      "#  Beta: -3.105335e-05\n",
      "#  Damping Parameter: 0.997134\n",
      "#  Gamma-1 Values: 7.169963e-08 9.876706e-08 1.339558e-07 7.005227e-08 6.118782e-08 1.315107e-07 2.855928e-07 1.608208e-07 1.141362e-07\n",
      "#  Gamma-2 Values: -1.070341e-07 6.63092e-08 -6.015409e-07 8.631575e-08 1.513088e-07 1.674032e-07 3.043384e-08 2.828268e-07 1.809086e-07\n",
      "#  AR coefficients: 0.985684\n",
      "\n",
      "# try to understand what these parameters mean!\n",
      "\n",
      "# The name of the forecasting method as a character string\n",
      "ha_tbats_many_forecast$method\n",
      "# [1] \"TBATS(1, {1,0}, 0.997, {<30,4>, <60,1>, <120,1>, <180,1>, <240,1>, <360,1>, <480,1>, <720,1>, <1440,1>})\"\n",
      "\n",
      "# Point forecasts as a time series\n",
      "length(ha_tbats_many_forecast$mean) \n",
      "#2880\n",
      "# This means we predict 2880 points into the future... Let's see what they are\n",
      "\n",
      "# Let's look at some sample forecasts...\n",
      "# I think this is per-minute data!!!\n",
      "# It predicts twice the largest frequency...\n",
      "ha_tbats_many_forecast$mean[1]\n",
      "ha_tbats_many_forecast$mean[10]\n",
      "ha_tbats_many_forecast$mean[50]\n",
      "ha_tbats_many_forecast$mean[100]\n",
      "ha_tbats_many_forecast$mean[200]\n",
      "ha_tbats_many_forecast$mean[350]\n",
      "ha_tbats_many_forecast$mean[900]\n",
      "ha_tbats_many_forecast$mean[1050]\n",
      "ha_tbats_many_forecast$mean[1400]\n",
      "ha_tbats_many_forecast$mean[1600]\n",
      "ha_tbats_many_forecast$mean[1800]\n",
      "ha_tbats_many_forecast$mean[2000]\n",
      "ha_tbats_many_forecast$mean[2200]\n",
      "ha_tbats_many_forecast$mean[2450]\n",
      "ha_tbats_many_forecast$mean[2850]\n",
      "\n",
      "ha_tbats_many_forecast$mean[1:80]\n",
      "\n",
      "# Extract components of a TBATS model\n",
      "# not completely sure what this does\n",
      "tbats.components(ha_tbats_many_fit)\n",
      "\n",
      "# let's look at forecast \n",
      "ha_tbats_many_forecast[0]\n",
      "ha_tbats_many_forecast[1]\n",
      "ha_tbats_many_forecast[2]\n",
      "ha_tbats_many_forecast[3]\n",
      "ha_tbats_many_forecast[4]\n",
      "ha_tbats_many_forecast[5]\n",
      "ha_tbats_many_forecast[6]\n",
      "\n",
      "\n",
      "ha_tbats_many_forecast_mean <- ha_tbats_many_forecast$mean\n",
      "length(ha_tbats_many_forecast_mean) # Has 2880 predictions\n",
      "\n",
      "# Get 95% upper forecasts\n",
      "ha_tbats_many_forecast_upper95 <- ha_tbats_many_forecast$upper[,2]\n",
      "length(ha_tbats_many_forecast_upper95) # Has 2880 predictions\n",
      "\n",
      "# quick test\n",
      "ha_tbats_many_forecast_upper95[10]\n",
      "\n",
      "# Get 80% upper forecasts\n",
      "ha_tbats_many_forecast_upper80 <- ha_tbats_many_forecast$upper[,1]\n",
      "length(ha_tbats_many_forecast_upper80) # Has 2880 predictions\n",
      "\n",
      "# quick test\n",
      "ha_tbats_many_forecast_upper80[10]\n",
      "\n",
      "# Get 95% lower forecasts\n",
      "ha_tbats_many_forecast_lower95 <- ha_tbats_many_forecast$lower[,2]\n",
      "length(ha_tbats_many_forecast_lower95) # Has 2880 predictions\n",
      "\n",
      "# quick test\n",
      "ha_tbats_many_forecast_lower95[10]\n",
      "\n",
      "# Get 80% lower forecasts\n",
      "ha_tbats_many_forecast_lower80 <- ha_tbats_many_forecast$lower[,1]\n",
      "length(ha_tbats_many_forecast_lower80) # Has 2880 predictions\n",
      "\n",
      "# quick test\n",
      "ha_tbats_many_forecast_lower80[10]\n",
      "\n",
      "################################################\n",
      "# Figure out how to extract time elements from last time: mdata_last_time\n",
      "# Then increment each forecast based on this last time\n",
      "\n",
      "# strftime: Date-time Conversion Functions to and from Character\n",
      "# %F is Equivalent to %Y-%m-%d (the ISO 8601 date format).\n",
      "# %T is Equivalent to %H:%M:%S.\n",
      "\n",
      "last_time <- strftime(mdata_last_time, format=\"%F %T\", tz=\"PST\")\n",
      "\n",
      "last <- toString(last_time)\n",
      "# \"2014-12-09 11:39:00\"\n",
      "\n",
      "# figure out type...\n",
      "typeof(get(ls()))\n",
      "\n",
      "# Let's try the lubridate package\n",
      "install.packages(\"lubridate\")\n",
      "\n",
      "library(\"lubridate\")\n",
      "\n",
      "# this only works when I convert to string\n",
      "last_lubri <- parse_date_time(last, \"%Y%m%d %H%M%S\", tz=\"PST\")\n",
      "\n",
      "# Yay! This is how we increment time in R()!\n",
      "last_lubri + 1*60\n",
      "last_lubri + 2*60\n",
      "last_lubri + 1440*60\n",
      "\n",
      "################################################\n",
      "\n",
      "# Create vector of times...\n",
      "vectimes <- function(x) last_lubri + x*60\n",
      "forecast_times <- vectimes( (1:2880) )\n",
      "\n",
      "# Now let's check forecast_times...\n",
      "length(forecast_times)\n",
      "min(forecast_times)\n",
      "max(forecast_times)\n",
      "forecast_times[2]\n",
      "forecast_times[2880]  \n",
      "\n",
      "forecast_df <- data.frame(forecast_times, ha_tbats_many_forecast_mean, ha_tbats_many_forecast_upper95, ha_tbats_many_forecast_lower95, ha_tbats_many_forecast_upper80, ha_tbats_many_forecast_lower80)\n",
      "\n",
      "forecast_df[0:5,]\n",
      "\n",
      "nrow(forecast_df) # confirms we have 2880 observations\n",
      "\n",
      "write.csv(forecast_df, file=\"ha_forecast_2880_dec2.csv\", row.names=F)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "forecast_multiple_algos.R: This script applies only one week of data (August 16-22, 2014) to forecast one day (August 23, 2014) using a number of forecasting models, including ARIMA, ETS, TBATS, Neural Nets, Cubic Splines, and Theta Method. I use built in accuracy function to compare eeach forecasting algorithm with actual data on August 23, 2014. From here, we select TBATS (best) and ARIMA (second-best) for visualization (see slides) based on RMSE (residual mean squared error) metric. Future work can incorporate more algorithms as well as different time scales for training data. We used August 16-22 because the pattern was likely less idiosyncratic than Dec 1-2, 2014 when Bob Bell had sickness that contributed to unusually high level of migraine."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Parallel\n",
      "library(\"parallel\")\n",
      "numCores = detectCores()\n",
      "\n",
      "# Load forecast library\n",
      "library(\"forecast\")\n",
      "\n",
      "# Read in one week (August 16-22, 2014) into mydata data frame\n",
      "mydata = read.csv(\"Headache.csv\")\n",
      "\n",
      "# mydata$time[233641] \n",
      "# Index for 2014-08-16 00:00:00-07:00\n",
      "start_index = 233641\n",
      "\n",
      "# mydata$time[243720]\n",
      "# Index for 2014-08-22 23:59:00-07:00\n",
      "stop_index = 243720\n",
      "\n",
      "# Here, create test index for assessing accuracy!\n",
      "# test_start_index starts immediately after stop_index\n",
      "test_start_index <- stop_index + 1\n",
      "\n",
      "# test_end_index is the 1440th test observation\n",
      "test_end_index   <- test_start_index + 1439\n",
      "\n",
      "# Extract latest time\n",
      "mdata_last_time <- mydata$time[stop_index]\n",
      "mdata_last_time\n",
      "\n",
      "# let's plot the headache level from start_index to stop_index\n",
      "plot(mydata$headache[start_index:stop_index])\n",
      "\n",
      "# Create headache as time series object with no frequency\n",
      "ha_ts <- ts(mydata$headache[start_index:stop_index])\n",
      "\n",
      "# Forecast: Let's get an automatic forecast\n",
      "# h=1440 specifies the number of periods for forecasting (here, 1440 periods/minutes is one day)\n",
      "# level=c(80,95) returns both 80% and 95% prediction intervals\n",
      "# find.frequency = TRUE uses the function to determine the appropriate period, if the data is of unknown\n",
      "# period\n",
      "ha_forecast <- forecast(ha_ts, h=1440, level=c(80,95), find.frequency=TRUE)\n",
      "\n",
      "# Let's do a plot of the forecast\n",
      "plot(ha_forecast)\n",
      "\n",
      "# Let's assess the accuracy\n",
      "# We compare the forecasts from ha_forecast with actual values from the next day\n",
      "\n",
      "auto_forecast_accuracy <- accuracy(ha_forecast, mydata$headache[test_start_index:test_end_index])\n",
      "auto_forecast_accuracy\n",
      "\n",
      "####################################################################\n",
      "# Here I fit the data to best ARIMA model\n",
      "# ha_ts is our time series\n",
      "ha_fit_arima <- auto.arima(ha_ts)\n",
      "\n",
      "ha_arima_forecast <- forecast(ha_fit_arima, h=1440)\n",
      "\n",
      "ha_arima_accuracy <- accuracy(ha_arima_forecast, mydata$headache[test_start_index:test_end_index])\n",
      "ha_arima_accuracy\n",
      "\n",
      "####################################################################\n",
      "# Neural Network Time Series Forecasts\n",
      "# ha_ts is time series\n",
      "ha_nnet_fit <- nnetar(ha_ts)\n",
      "\n",
      "# Forecast 1440 periods based on\n",
      "ha_nnet_forecast <- forecast(ha_nnet_fit, h=1440)\n",
      "\n",
      "ha_nnet_accuracy <- accuracy(ha_nnet_forecast, mydata$headache[test_start_index:test_end_index])\n",
      "ha_nnet_accuracy\n",
      "\n",
      "####################################################################\n",
      "# Exponential smoothing state space model\n",
      "# ha_ts is time series\n",
      "ha_ets_fit <- ets(ha_ts)\n",
      "\n",
      "# Forecast 1440 periods based on\n",
      "ha_ets_forecast <- forecast(ha_ets_fit, h=1440)\n",
      "\n",
      "ha_ets_accuracy <- accuracy(ha_ets_forecast, mydata$headache[test_start_index:test_end_index])\n",
      "ha_ets_accuracy\n",
      "\n",
      "####################################################################\n",
      "# thetaf Theta method forecast\n",
      "# ha_ts is time series\n",
      "# 80 and 95% prediction intervals with 1440 periods of forecasting\n",
      "ha_thetaf_forecast <- thetaf(ha_ts, level=c(80,95), h=1440)\n",
      "\n",
      "ha_thetaf_accuracy <- accuracy(ha_thetaf_forecast, mydata$headache[test_start_index:test_end_index])\n",
      "ha_thetaf_accuracy\n",
      "\n",
      "\n",
      "####################################################################\n",
      "# TBATS model (Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components)\n",
      "# ha_ts is time series (note, this is NOT msts object)\n",
      "# We parallelize with numCores (4)\n",
      "ha_tbats_fit <- tbats(ha_ts, use.parallel=TRUE, num.cores=numCores)\n",
      "\n",
      "# Forecast 1440 periods\n",
      "ha_tbats_forecast <- forecast(ha_tbats_fit, h=1440)\n",
      "\n",
      "ha_tbats_accuracy <- accuracy(ha_tbats_forecast, mydata$headache[test_start_index:test_end_index])\n",
      "ha_tbats_accuracy\n",
      "\n",
      "\n",
      "####################################################################\n",
      "# splinef: Cubic Spline Forecast\n",
      "# ha_ts is time series\n",
      "# 80 and 95% prediction intervals with 1440 periods of forecasting\n",
      "ha_splinef_forecast <- splinef(ha_ts, level=c(80,95), h=1440)\n",
      "\n",
      "ha_splinef_accuracy <- accuracy(ha_splinef_forecast, mydata$headache[test_start_index:test_end_index])\n",
      "ha_splinef_accuracy\n",
      "\n",
      "\n",
      "####################################################################\n",
      "# rwf: Random Walk Forecast\n",
      "# ha_ts is time series\n",
      "# 80 and 95% prediction intervals with 1440 periods of forecasting\n",
      "ha_rwf_forecast <- rwf(ha_ts, level=c(80,95), h=1440)\n",
      "\n",
      "ha_rwf_accuracy <- accuracy(ha_rwf_forecast, mydata$headache[test_start_index:test_end_index])\n",
      "ha_rwf_accuracy\n",
      "\n",
      "####################################################################\n",
      "Now let's print out all accuracies...\n",
      "'\n",
      "\n",
      "\n",
      "auto_forecast_accuracy\n",
      "ha_arima_accuracy\n",
      "ha_nnet_accuracy\n",
      "ha_ets_accuracy\n",
      "ha_thetaf_accuracy\n",
      "ha_tbats_accuracy\n",
      "ha_rwf_accuracy\n",
      "\n",
      "\n",
      "\n",
      "####################################################################\n",
      "# Let's print out arima forecasts for CSV...\n",
      "\n",
      "#mdata_last_time <- mydata$time[stop_index]\n",
      "\n",
      "#mdata_last_time\n",
      "\n",
      "#mdata_last_time <- toString(mdata_last_time)\n",
      "\n",
      "#last_time <- strftime(mdata_last_time, format=\"%F %T\")\n",
      "#last_time\n",
      "\n",
      "#last <- toString(last_time)\n",
      "# \"2014-12-09 11:39:00\"\n",
      "\n",
      "# figure out type...\n",
      "#typeof(get(ls()))\n",
      "\n",
      "# Let's try the lubridate package\n",
      "#install.packages(\"lubridate\")\n",
      "\n",
      "#library(\"lubridate\")\n",
      "\n",
      "# this only works when I convert to string\n",
      "#last_lubri <- parse_date_time(last, \"%Y%m%d %H%M%S\", tz=\"PST\")\n",
      "\n",
      "# Yay! This is how we increment time in R()!\n",
      "#last_lubri + 1*60\n",
      "#last_lubri + 2*60\n",
      "#last_lubri + 1440*60\n",
      "\n",
      "\n",
      "\n",
      "forecast_df <- data.frame(ha_arima_forecast$mean, ha_arima_forecast$upper[,2], ha_arima_forecast$lower[,2], ha_arima_forecast$upper[,1], ha_arima_forecast$lower[,1], mydata$headache[test_start_index:test_end_index])\n",
      "\n",
      "length(ha_arima_forecast$mean)\n",
      "length(ha_arima_forecast$upper[,2]) # upper 95\n",
      "length(ha_arima_forecast$lower[,2]) # lower 95\n",
      "length(ha_arima_forecast$upper[,1]) # upper 80\n",
      "length(ha_arima_forecast$lower[,1]) # lower 80\n",
      "length(mydata$headache[test_start_index:test_end_index])\n",
      "\n",
      "forecast_df[0:5,]\n",
      "\n",
      "nrow(forecast_df) # confirms we have 1440 observations\n",
      "\n",
      "write.csv(forecast_df, file=\"ha_forecast_1440_aug23_arima.csv\", row.names=F)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# find the model with the best accuracy...\n",
      "# then look at its numbers...\n",
      "\n",
      "# Alternatively try this for 2 weeks prior to a day of forecasting\n",
      "# when i have more time, find the optimal time window through cross-validation?\n",
      "# Or maybe there is some theory behind this?\n",
      "\n",
      "\n",
      "# OR just stop with this??? and evaluate accuracy on all models, including tbats...\n",
      "# And just report that... in addition to my other forecasts???\n",
      "\n",
      "# What else to consider\n",
      "# tsoutliers: Identify and replace outliers in a time series\n",
      "# tsdisplay(ha_ts): exploratory function showing auto-correlation\n",
      "# tsclean: Identify and replace outliers and missing values in a time series... Maybe apply if I have outliers?\n",
      "# BoxCox.lambda Automatic selection of Box Cox transformation parameter\n",
      "\n",
      "\n",
      "\n",
      "#################################################################################\n",
      "# let's try dshw\n",
      "# First create msts object with only 2 frequencies - 60 (hour) and 1440 (day)\n",
      "ha_msts_hr_day <- msts(mydata$headache, seasonal.periods=c(60, 1440))\n",
      "ha_dshw_fit <- dshw(ha_msts_hr_day)\n",
      "# dshw not suitable when data contain zeros or negative numbers\n",
      "\n",
      "#################################################################################\n",
      "# let's try stlf\n",
      "# first a ts object with frequency 60\n",
      "ha_ts_hr <- ts(mydata$headache, frequency=60)\n",
      "ha_stlf_fit <- stlf(ha_ts_hr)\n",
      "summary(ha_stlf_fit)\n",
      "\n",
      "\n",
      "\n",
      "#################################################################################\n",
      "# Create msts() object for headache...\n",
      "# This means that I can have multiple frequencies: hour (60), day (1440), and week(10080)\n",
      "ha_msts <- msts(mydata$headache, seasonal.periods=c(60, 1440, 10080), ts.frequency=60)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#################################################################################\n",
      "# Use simple vector of numbers.. ANd forecast 1440 points in advance with splinef\n",
      "ha_splinf_fit <- splinef(mydata$headache)\n",
      "\n",
      "#Error: cannot allocate vector of size 1192.1 Gb\n",
      "#In addition: Warning messages:\n",
      "#  1: In matrix(0, nrow = nn, ncol = nn) :\n",
      "#  Reached total allocation of 15842Mb: see help(memory.size)\n",
      "#2: In matrix(0, nrow = nn, ncol = nn) :\n",
      "#  Reached total allocation of 15842Mb: see help(memory.size)\n",
      "#3: In matrix(0, nrow = nn, ncol = nn) :\n",
      "#  Reached total allocation of 15842Mb: see help(memory.size)\n",
      "#4: In matrix(0, nrow = nn, ncol = nn) :\n",
      "#  Reached total allocation of 15842Mb: see help(memory.size)\n",
      "\n",
      "\n",
      "# now let's try a ts object with frequency 60\n",
      "ha_ts_hr <- ts(mydata$headache, frequency=60)\n",
      "ha_splinf_fit <- splinef(ha_ts_hr)\n",
      "\n",
      "#Error: cannot allocate vector of size 1192.1 Gb\n",
      "#In addition: Warning messages:\n",
      "#  1: In matrix(0, nrow = nn, ncol = nn) :\n",
      "#  Reached total allocation of 15842Mb: see help(memory.size)\n",
      "#2: In matrix(0, nrow = nn, ncol = nn) :\n",
      "#  Reached total allocation of 15842Mb: see help(memory.size)\n",
      "#3: In matrix(0, nrow = nn, ncol = nn) :\n",
      "#  Reached total allocation of 15842Mb: see help(memory.size)\n",
      "#4: In matrix(0, nrow = nn, ncol = nn) :\n",
      "#  Reached total allocation of 15842Mb: see help(memory.size)\n",
      "\n",
      "\n",
      "\n",
      "#################################################################################\n",
      "# Let's try time series with frequency of 12 (12 minute intervals)\n",
      "# And let's apply ets() to this\n",
      "\n",
      "# ts object with frequency 12\n",
      "ha_ts_twelve <- ts(mydata$headache, frequency=12)\n",
      "ha_ets_fit <- ets(ha_ts_twelve)\n",
      "\n",
      "# If this works, plot the forecasts!\n",
      "plot(forecast(ha_ets_fit, h=1400))\n",
      "\n",
      "# Explore this later!\n",
      "\n",
      "\n",
      "# Re-apply ets to simple vector and see what happens\n",
      "ha_ets_fit_vector <- ets(mydata$headache)\n",
      "plot(forecast(ha_ets_fit_vector))\n",
      "\n",
      "######################################################################################\n",
      "# Create msts() object for headache...\n",
      "# This means that I can have multiple frequencies: hour (60) and day (1440)\n",
      "ha_msts_hr_day <- msts(mydata$headache, seasonal.periods=c(60, 1440))\n",
      "\n",
      "# Fit TBATS on data with hourly and daily seasons\n",
      "\n",
      "ha_tbats_hr_day_fit <- tbats(ha_msts_hr_day)\n",
      "\n",
      "# Now this looks interesting: \n",
      "plot(forecast(ha_tbats_hr_day_fit))\n",
      "\n",
      "summary(ha_tbats_hr_day_fit)\n",
      "\n",
      "ha_tbats_hr_day_forecast <- forecast.tbats(ha_tbats_hr_day_fit) # this works...\n",
      "\n",
      "ha_tbats_hr_day_forecast$model\n",
      "length(ha_tbats_hr_day_forecast$mean) # this returns 2880 values...\n",
      "\n",
      "# this gives h=2880, which is twice the largest frequency (1440)...\n",
      "\n",
      "length(ha_tbats_hr_day_forecast$lower) # lower gives 5760 (probably two lower bounds)\n",
      "\n",
      "length(ha_tbats_hr_day_forecast$upper) # upper gives 5760 (probably two upper bounds)\n",
      "\n",
      "ha_tbats_hr_day_forecast$level # returns 80 95\n",
      "\n",
      "ha_tbats_hr_day_forecast\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "###################################################################################\n",
      "# Let's do tbats with a crazy msts time series involving the following frequencies:\n",
      "# freq=30 (half-hour), 60 (hour), 120 (2 hours), 180 (3 hours), 240 (4 hours), \n",
      "# 360 (6 hours), 480 (8 hours), 720 (12 hours), and 1440 (day).\n",
      "\n",
      "ha_msts_many <- msts(mydata$headache, seasonal.periods=c(30, 60, 120, 180, 240, 360, 480, 720, 1440))\n",
      "\n",
      "library(\"parallel\")\n",
      "# library(help = \"parallel\")\n",
      "numCores = detectCores()\n",
      "\n",
      "# Fit TBATS on data with hourly and daily seasons\n",
      "\n",
      "ha_tbats_many_fit <- tbats(ha_msts_many, use.parallel=TRUE, num.cores=numCores)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# Next would be msts object of only 60 and 1440 periods with bats/tbats\n",
      "# Another would be msts object with 60 (per hour) and 120 (per two hours) and maybe (480)\n",
      "# Another would be ts object with 120 (per two hours) applied to arima\n",
      "\n",
      "# Also do a simple forecast (fully automated) with vector (no frequency) and freq=12, 60, \n",
      "# 120, 240, 480, 960 and 1440.\n",
      "# Exhaust all possibilities with this package!\n",
      "\n",
      "\n",
      "# Finally, figure out how to do the following\n",
      "# (1) RStudio Shiny (on this computer), including running in the cloud. Anyway to\n",
      "# visualize this?\n",
      "# (2) Run R Scripts on Sense.io as well as DOmino Data Lab - increasing clusters \n",
      "# and automated processes\n",
      "# (3) Auto-updating headache level from CSV/Google calendar\n",
      "# (4) Re-run Variable importance analysis (with weather data?) on cloud services? and for \n",
      "# 2hr, 8hr, and 24hr data.\n",
      "# (5) Consider alternative variable importance measures, including tmle.npvi and RF for\n",
      "# comparison...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Convert headache to a time series\n",
      "# ha_ts <- ts(mydata$headache, start=c(2014, 3), frequency=1)\n",
      "ha_ts <- ts(mydata$headache) # Will this work?\n",
      "plot(ha_ts) # Simple plot of headache data\n",
      "hist(ha_ts) # histogram of headache data\n",
      "\n",
      "\n",
      "# Automated forecasting using an exponential model\n",
      "headache_ets <- ets(ha_ts)\n",
      "\n",
      "# predictive accuracy for headache_ets\n",
      "accuracy(headache_ets)\n",
      "\n",
      "# predict next 10 observations with ets model\n",
      "forecast(headache_ets, 10, level=95)\n",
      "plot(forecast(headache_ets, 10))\n",
      "\n",
      "# Automated forecasting using an ARIMA model\n",
      "headache_arima <- auto.arima(ha_ts)\n",
      "\n",
      "# predictive accuracy for headache_arima\n",
      "accuracy(headache_arima)\n",
      "\n",
      "# predict next 10 observations with arima model\n",
      "forecast(headache_arima, 150, level=95)\n",
      "plot(forecast(headache_arima, 10:11))\n",
      "\n",
      "summary(headache_arima)\n",
      "\n",
      "\n",
      "\n",
      "# # Automated forecasting using best package\n",
      "# headache_auto <- forecast(ha_ts, 10000) # Asking for forecast on next 1440 time points...\n",
      "\n",
      "# What happens if we just ask for forecast???\n",
      "headache_auto <- forecast(ha_ts, h=1000) \n",
      "\n",
      "\n",
      "summary(headache_auto)\n",
      "\n",
      "accuracy(headache_auto)\n",
      "\n",
      "\n",
      "\n",
      "# What else can I do with ts object?\n",
      "\n",
      "\n",
      "# Yay! this works! I just need to figure out the time scales in Python...\n",
      "plot(forecast(ha_ts))\n",
      "\n",
      "# Alternatively creating a forecast object called ha_forecast.\n",
      "plot(forecast(ha_ts, h=1440))\n",
      "\n",
      "daily_headache_forecast <- forecast(ha_ts, h=1440)\n",
      "\n",
      "plot(daily_headache_forecast)\n",
      "\n",
      "summary(daily_headache_forecast)\n",
      "\n",
      "accuracy(daily_headache_forecast)\n",
      "\n",
      "plot.forecast(daily_headache_forecast)\n",
      "\n",
      "daily_headache_forecast$fitted[1000]\n",
      "\n",
      "# I am getting some values. I just need to figure out what are the actual time points.\n",
      "# Also, should I specify a number to predict into the future with or not???"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}