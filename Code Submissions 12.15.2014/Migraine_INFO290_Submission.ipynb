{
 "metadata": {
  "name": "",
  "signature": "sha256:de6ab16051b220cdaf8637f13089b4eca64d5ad33b727ea53334c30f9b495a16"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Migraine Project"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Here we import all all Python-related modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import modules\n",
      "\n",
      "# Import zipfile so I can unzip Dropbox files\n",
      "import zipfile\n",
      "\n",
      "# Import StringIO so I can more easily convert CSV files to data frames\n",
      "import StringIO\n",
      "\n",
      "# Import Python regular expression module\n",
      "import re\n",
      "\n",
      "# Import Google API Python Client Modules\n",
      "from apiclient.discovery import build\n",
      "from oauth2client.client import OAuth2WebServerFlow\n",
      "from oauth2client.file import Storage\n",
      "from oauth2client.tools import run\n",
      "import httplib2\n",
      "import gflags\n",
      "\n",
      "# Import Pybasis module\n",
      "import pybasis\n",
      "\n",
      "# Importing pandas, numpy, matplotlib\n",
      "from pandas import Series, DataFrame, read_csv\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "% matplotlib inline\n",
      "\n",
      "# Importing weather data library\n",
      "import pywapi\n",
      "import string\n",
      "\n",
      "# Date/time modules\n",
      "from datetime import datetime, timedelta, time\n",
      "from dateutil import parser\n",
      "# import time # do I need this?\n",
      "\n",
      "# Include the Dropbox SDK\n",
      "import dropbox\n",
      "\n",
      "# Math module\n",
      "import math\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Here, we experiment with accessing SMS and call logs. We also did a sentiment analysis on sms text. This is for future work and is not incorporated in analysis for this course."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zf = zipfile.ZipFile('Backup_2014-11-30 05-02-35.zip', 'r')\n",
      "# New file is 'Backup_2014-12-02 21-42-35.zip'\n",
      "print zf.namelist()\n",
      "if ('SmsBackup.csv' in zf.namelist()) and ('CallLogBackup.csv' in zf.namelist()) and ('Calendar.csv' in zf.namelist()):\n",
      "    print \"All files are there\"\n",
      "\n",
      "# Read contents of unzipped csv and place into string sms_Data\n",
      "sms_Data = zf.read('SmsBackup.csv')\n",
      "\n",
      "# Read contents of unzipped csv and place into string call_Log\n",
      "call_Log = zf.read('CallLogBackup.csv')\n",
      "\n",
      "# Unfortunately, call_Log has a missing column.\n",
      "# Manually insert column name 'misc' into header row to correct for this\n",
      "# First, find where I should insert the new column name\n",
      "insert_index = call_Log.find('voicemail_uri') + len('voicemail_uri')\n",
      "# Now insert column name 'misc' into the header row\n",
      "new_call_Log = call_Log[:insert_index] + ',misc' + call_Log[insert_index:len(call_Log)+1]\n",
      "\n",
      "# Turn string types into file formats that pandas can read as a CSV\n",
      "smsFile  = StringIO.StringIO(sms_Data)\n",
      "callFile = StringIO.StringIO(new_call_Log)\n",
      "\n",
      "# Turn smsFile into pandas dataframe smsframe\n",
      "smsframe   = pd.read_csv(smsFile)\n",
      "\n",
      "# Turn callFile into pandas dataframe callframe\n",
      "callframe  = pd.read_csv(callFile)\n",
      "\n",
      "# Now let's find way to extract the data we want to put in our big dataframe\n",
      "\n",
      "# SMS. I only care about following columns: Date/Time, Name, read, and Message\n",
      "# read=1 means that I interacted with the message in some way...\n",
      "# what is the goal here? Count numbers of positive and negative text messages\n",
      "# I should have a running total of positive/negative text messages.\n",
      "# For example, txt \n",
      "\n",
      "\n",
      "# Algorithm\n",
      "# (0) First, set max date/time = March 6, 2014 @ 12:00:00\n",
      "# (1) Turn Date/Time element into a series/time element.\n",
      "# (2) For the entry, extract message.\n",
      "# (3) Do sentiment analysis via NLTK for now...\n",
      "# (4) Store this sentiment into big data frame at select date/time\n",
      "# (5) GO through data frame and determine max date/time. Can use this to streamline for the future...\n",
      "\n",
      "# Start with March 6, 2014\n",
      "latest_sms_dt = datetime.strptime(\"2014-11-06 00:00:00\", '%Y-%m-%d %H:%M:%S')\n",
      "\n",
      "from textblob import TextBlob\n",
      "\n",
      "for i in range(len(smsframe['Date/Time'])):\n",
      "    cur_sms_dt = datetime.strptime(smsframe['Date/Time'][i], '%Y-%m-%d %H:%M:%S')\n",
      "    \n",
      "    #Check that current SMS message is new and that it has been read...\n",
      "    if (cur_sms_dt > latest_sms_dt) and (smsframe['read'][i] == 1):\n",
      "        Message = str(smsframe['Message'][i])\n",
      "\n",
      "        # Manually remove weird unicode characters\n",
      "        Message = Message.decode('utf-8')\n",
      "        \n",
      "        # Convert to TextBlob object for sentiment analysis \n",
      "        blob = TextBlob(Message)\n",
      "        \n",
      "        # Polarity gets sentiment analysis for each message\n",
      "        polarity = str(blob.sentiment.polarity)\n",
      "        \n",
      "        # Print date/time, polarity, and message\n",
      "        print str(cur_sms_dt) + '|||' +  polarity + '|||' + smsframe['Message'][i]\n",
      "\n",
      "# Find the latest time I ever sent/received a text message\n",
      "latest_sms_dt_str = max(smsframe['Date/Time'])\n",
      "\n",
      "# Convert this string into a date-time object\n",
      "latest_sms_dt = datetime.strptime(latest_sms_dt_str, '%Y-%m-%d %H:%M:%S')\n",
      "\n",
      "print latest_sms_dt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now let's extract data from Mobile Phone Call Log\n",
      "\n",
      "# Call Log. I only care about following columns: Date/Time, Duration, Type, and Name\n",
      "# Type != \"Missed\" means that I interacted with someone one an incoming or outgoing phone call...\n",
      "# what is the goal here? Create Binary Columns for conversations with people.\n",
      "# Eventually, I want dataframe to have columns like: Call_Frankie; Call_Warigia; Call_Kevan_Peabody; etc.\n",
      "# If all Call_YYY columns have value 0, then I am not on a phone call...\n",
      "\n",
      "\n",
      "# Algorithm\n",
      "# (0) First, set max date/time = March 6, 2014 @ 12:00:00\n",
      "# (1) Turn Date/Time element into a series/time element.\n",
      "# (2) For each entry, extract duration time and Name\n",
      "# (3) Store this sentiment into big data frame at select date/time\n",
      "# (5) Go through data frame and determine max date/time. Can use this to streamline for the future...\n",
      "\n",
      "# Start with March 6, 2014\n",
      "# Eventually, store this in Dropbox as a file! Keep a counter of latest changes.\n",
      "latest_call_dt = datetime.strptime(\"2014-11-06 00:00:00\", '%Y-%m-%d %H:%M:%S')\n",
      "\n",
      "for i in range(len(callframe['Date/Time'])):\n",
      "    cur_call_dt = datetime.strptime(callframe['Date/Time'][i], '%Y-%m-%d %H:%M:%S')\n",
      "    \n",
      "    #Check that current call is new and that it is not missed...\n",
      "    if (cur_call_dt > latest_call_dt) and (callframe['Type'][i] != \"Missed\"):\n",
      "\n",
      "        Call_Name = str(callframe['Name'][i])\n",
      "        # Manually remove weird unicode characters\n",
      "        Call_Name = Call_Name.decode('utf-8')\n",
      "        \n",
      "        Call_Duration = str(callframe['Duration'][i])\n",
      "        Call_Duration = Call_Duration.decode('utf-8')\n",
      "        \n",
      "        call_delta_hours = call_delta_minutes = call_delta_seconds = 0\n",
      "\n",
      "        # Following code parses Duration String for hours, minutes, and seconds\n",
      "        # Returns call_delta_hours, call_delta_minutes, and call_delta_seconds\n",
      "        if re.search(\"\\s*h?\\s*m?\\s*s\\s*\", Call_Duration):       \n",
      "            call_delta = re.split(\"\\s*h?\\s*m?\\s*s?\\s*\", Call_Duration)\n",
      "            if len(call_delta) == 2:\n",
      "                call_delta_seconds = int(call_delta[0])\n",
      "            if len(call_delta) == 3:\n",
      "                call_delta_minutes = int(call_delta[0])\n",
      "                call_delta_seconds = int(call_delta[1])\n",
      "            if len(call_delta) == 4:\n",
      "                call_delta_hours   = int(call_delta[0])\n",
      "                call_delta_minutes = int(call_delta[1])\n",
      "                call_delta_seconds = int(call_delta[2])\n",
      "\n",
      "        # Put this extracted data into a timedelta object\n",
      "        call_delta = timedelta(hours = call_delta_hours, minutes = call_delta_minutes, seconds = call_delta_seconds)\n",
      "\n",
      "        # I used .seconds because .min did not work.\n",
      "        # Added 1 to delta_min\n",
      "        call_delta_min = int((call_delta.seconds / 60) + 1)\n",
      "        \n",
      "        # Print date/time, Duration, and Name\n",
      "        print str(cur_call_dt) + '|||' +  Call_Name + '|||' + str(call_delta_min)\n",
      "\n",
      "# Find the latest time I ever sent/received a text message\n",
      "latest_call_dt_str = max(callframe['Date/Time'])\n",
      "\n",
      "# Convert this string into a date-time object\n",
      "latest_call_dt = datetime.strptime(latest_call_dt_str, '%Y-%m-%d %H:%M:%S')\n",
      "\n",
      "print \"latest call time was: \" + str(latest_call_dt)\n",
      "\n",
      "\n",
      "# I used .seconds because .min did not work.\n",
      "# Added 1 to delta_min\n",
      "\n",
      "# delta_min = int((gcal_time_duration.seconds / 60) + 1)\n",
      "\n",
      "# Print delta_min as sanity check\n",
      "# print delta_min\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hold off on calendar data for now... Just stick with Google Calendar right now!\n",
      "# Read contents of unzipped csv and place into string cal_Data\n",
      "# cal_Data = zf.read('Calendar.csv')\n",
      "# print cal_Data[0:2000]\n",
      "# Unfortunately, calendar string needs to be manually formatted.\n",
      "# The header row unfortunately adds extra new lines\n",
      "\n",
      "#cal_index1  = cal_Data.find('cal_sync10\\nEvents_Col')\n",
      "#cal_offset1 = len('cal_sync10\\nEvents_Col')\n",
      "\n",
      "#cal_index2  = cal_Data.find('EndTimezone\\nAttendees_Col')\n",
      "#cal_offset2 = len('EndTimezone\\nAttendees_Col')\n",
      "\n",
      "#cal_index3  = cal_Data.find('attendeeType\\nReminders_Col')\n",
      "#cal_offset3 = len('attendeeType\\nReminders_Col')\n",
      "\n",
      "#new_cal_Data = cal_Data[:cal_index1] + 'cal_sync10,Events_Col' + cal_Data[cal_index1+cal_offset1:cal_index2] + 'EndTimezone,Attendees_Col' + cal_Data[cal_index2+cal_offset2:cal_index3] + 'attendeeType,Reminders_Col' + cal_Data[cal_index3+cal_offset3:]\n",
      "\n",
      "#cal_Data[0:cal_Data.find('cal_sync10\\nEvents_Col')]\n",
      "\n",
      "#print new_cal_Data[0:2550]\n",
      "\n",
      "\n",
      "#calFile  = StringIO.StringIO(cal_Data)\n",
      "\n",
      "# Turn calFile into pandas dataframe calenframe\n",
      "#calenframe = pd.read_csv(calFile)\n",
      "\n",
      "#calenframe[][1000:1010]\n",
      "\n",
      "#smsframe['Date/Time'][0]\n",
      "\n",
      "#frame['Name'][0]\n",
      "#print callframe['Name'][0]\n",
      "\n",
      "\n",
      "\n",
      "#frame = pd.read_csv(StringIO(smsData))\n",
      "\n",
      "\n",
      "#print sms[1000:1900]\n",
      "\n",
      "#print type(sms)\n",
      "\n",
      "#sms_df = pd.read_csv(sms)\n",
      "\n",
      "#print type(sms_df)\n",
      "#print sms_df.Name(10)\n",
      "\n",
      "\n",
      "#sms_df.head()\n",
      "\n",
      "\n",
      "#data_df = pd.read_csv(zf.read('SmsBackup.csv'))\n",
      "\n",
      "\n",
      "#print type(smsData)\n",
      "\n",
      "#df_sms = DataFrame.from_csv(smsData, encoding='utf8')\n",
      "\n",
      "#sms = pd.read_csv(smsData)\n",
      "\n",
      "#df = pd.read_csv(smsData)\n",
      "#\n",
      "\n",
      "#head(df)\n",
      "\n",
      "#import csv "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "We initialize Google API for calendar access"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Google Calendar API Authorization\n",
      "\n",
      "FLAGS = gflags.FLAGS\n",
      "\n",
      "# Set up a Flow object to be used if we need to authenticate. This\n",
      "# sample uses OAuth 2.0, and we set up the OAuth2WebServerFlow with\n",
      "# the information it needs to authenticate. Note that it is called\n",
      "# the Web Server Flow, but it can also handle the flow for native\n",
      "# applications\n",
      "# The client_id and client_secret can be found in Google Developers Console\n",
      "FLOW = OAuth2WebServerFlow(\n",
      "    # client_id=,\n",
      "    # client_secret=,\n",
      "    # scope='https://www.googleapis.com/auth/calendar',\n",
      "    # user_agent='INFO290'\n",
      "    )\n",
      "\n",
      "# Note these credentials are from \"Client ID for native application\"\n",
      "\n",
      "# To disable the local server feature, uncomment the following line:\n",
      "# FLAGS.auth_local_webserver = False\n",
      "\n",
      "# If the Credentials don't exist or are invalid, run through the native client\n",
      "# flow. The Storage object will ensure that if successful the good\n",
      "# Credentials will get written back to a file.\n",
      "storage = Storage('calendar.dat')\n",
      "credentials = storage.get()\n",
      "if credentials is None or credentials.invalid == True:\n",
      "  credentials = run(FLOW, storage)\n",
      "\n",
      "# Create an httplib2.Http object to handle our HTTP requests and authorize it\n",
      "# with our good Credentials.\n",
      "http = httplib2.Http()\n",
      "http = credentials.authorize(http)\n",
      "\n",
      "# Build a service object for interacting with the API. Visit\n",
      "# the Google Developers Console\n",
      "# to get a developerKey for your own application.\n",
      "service = build(serviceName='calendar', version='v3', http=http,\n",
      "       developerKey='YOUR_DEVELOPER_KEY')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Authorize API access to Basis Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Authorize pyBasis API\n",
      "# basisAPI = pybasis.basisAPI(\"\",\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Create time series data frame (with pandas) for headache data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create Empty Dataframe\n",
      "\n",
      "# Create time index starting with March 6, 2014 @ 2am with frequency of 1 min\n",
      "index=pd.date_range('20140306T17:00:00',periods=400000,freq='1min', tz='US/Pacific')\n",
      "#index=pd.date_range('20140306T17:00:00',periods=11000,freq='1min', tz='US/Pacific')\n",
      "# Let's shorten for now!\n",
      "\n",
      "# check to verify time zone is US/Pacific\n",
      "print(index.tz)\n",
      "\n",
      "# Check to verify type for pandas index is timestamp\n",
      "print type(index[0])\n",
      "\n",
      "# Create column names reflecting data categories for collection\n",
      "# cols = ['time', 'skin_temp', 'heartrate', 'air_temp', 'calories', 'gsr', 'steps', 'act_type', 'sleep_type', 'toss_turn', 'calendar_summary1', 'calendar_summary2', 'calendar_summary3', 'calendar_summary4', 'kt', 'food', 'food_ingredients', 'time_since_food', 'drink', 'drink_ingredients', 'time_since_drink', 'headache', 'medicine', 'medicine_ingredients','glucose', 'relaxation', 'meditation', 'eye_contacts', 'fitness', 'odor', 'calendar_event1', 'calendar_event2', 'calendar_event3', 'latitude', 'longitude', 'elevation', 'accuracy', 'bearing', 'speed', 'call_duration', 'call_type', 'call_number', 'call_name', 'sms_number', 'sms_name', 'sms_type', 'sms_message', 'sms_seen', 'location_name', 'last_updated', 'temp', 'text', 'uv_index', 'uv_text', 'visibility', 'humidity', 'barometer_direction', 'barometer_reading', 'feels_like', 'wind_gust', 'wind_direction', 'wind_speed', 'wind_text', 'dewpoint', 'moonphase_icon', 'moonphase_text']\n",
      "\n",
      "# cols = ['time', 'skin_temp', 'heartrate', 'air_temp', 'calories', 'gsr', 'steps', 'act_type', 'sleep_type', 'toss_turn', 'calendar_summary1', 'calendar_summary2', 'calendar_summary3', 'calendar_summary4', 'kt', 'food', 'time_since_food', 'drink', 'time_since_drink', 'headache', 'medicine', 'glucose', 'relaxation', 'meditation', 'eye_contacts', 'fitness', 'odor', 'calendar_event1', 'calendar_event2', 'calendar_event3', 'latitude', 'longitude', 'elevation', 'accuracy', 'bearing', 'speed', 'call_duration', 'call_type', 'call_number', 'call_name', 'sms_number', 'sms_name', 'sms_type', 'sms_message', 'sms_seen', 'location_name', 'last_updated', 'temp', 'text', 'uv_index', 'uv_text', 'visibility', 'humidity', 'barometer_direction', 'barometer_reading', 'feels_like', 'wind_gust', 'wind_direction', 'wind_speed', 'wind_text', 'dewpoint', 'moonphase_icon', 'moonphase_text', 'phone_call', 'text_msg', 'text_msg_sentiment', 'sickness', 'flight', 'medical', 'dissertation', 'odor', 'berkeley', 'barber', 'teeth', 'meetup', 'class_time', 'meditation', 'chocolate', 'soup', 'dairy', 'citrus', 'gluten', 'onion', 'pickles', 'olives', 'garlic', 'beans', 'banana', 'avocado', 'papaya', 'alcohol', 'raisin', 'nuts', 'pain_med', 'ausanil', 'tramadol', 'antibiotic', 'hbr', 'benadryl', 'neilmed', 'herbal_med', 'flonase', 'magnesium', 'pj', 'anefrin', 'acetometophin', 'supp_med', 'motrin', 'ginkgo', 'aleve', 'allegra', 'water', 'melatonin', 'htp', 'fish_oil', 'niacin', 'coq10', 'zinc', 'glucosamine', 'chondroitin', 'vit_c', 'vit_d', 'butterbur', 'feverfew', 'dymista', 'topical_analgesic', 'bc_powder', 'montelukast', 'zyrtec', 'multivitamin', 'riboflavin', 'chromium', 'tb', 'cough_drop', 'caffeine', 'loratadine', 'maxalt', 'zomig', 'hedelman2', 'ahope2', 'ahope1', 'allergy_med', 'test_boost', 'creatine', 'aura', 'drink_indicator', 'food_indicator', 'fitness_indicator', 'vacation', 'prunes', 'odor_indicator', 'meditation_indicator']\n",
      "\n",
      "cols = ['time', 'skin_temp', 'heartrate', 'air_temp', 'calories', 'gsr', 'steps', 'act_type', 'sleep_type', 'toss_turn', 'calendar_summary1', 'calendar_summary2', 'calendar_summary3', 'calendar_summary4', 'kt', 'food', 'time_since_food', 'drink', 'time_since_drink', 'headache', 'medicine', 'glucose', 'relaxation', 'meditation', 'eye_contacts', 'fitness', 'odor', 'calendar_event1', 'calendar_event2', 'calendar_event3', 'sickness', 'flight', 'medical', 'dissertation', 'odor', 'berkeley', 'barber', 'teeth', 'meetup', 'class_time', 'meditation', 'chocolate', 'soup', 'dairy', 'citrus', 'gluten', 'onion', 'pickles', 'olives', 'garlic', 'beans', 'banana', 'avocado', 'papaya', 'alcohol', 'raisin', 'nuts', 'pain_med', 'ausanil', 'tramadol', 'antibiotic', 'hbr', 'benadryl', 'neilmed', 'herbal_med', 'flonase', 'magnesium', 'pj', 'anefrin', 'acetometophin', 'supp_med', 'motrin', 'ginkgo', 'aleve', 'allegra', 'water', 'melatonin', 'htp', 'fish_oil', 'niacin', 'coq10', 'zinc', 'glucosamine', 'chondroitin', 'vit_c', 'vit_d', 'butterbur', 'feverfew', 'dymista', 'topical_analgesic', 'bc_powder', 'montelukast', 'zyrtec', 'multivitamin', 'riboflavin', 'chromium', 'tb', 'cough_drop', 'caffeine', 'loratadine', 'maxalt', 'zomig', 'hedelman2', 'ahope2', 'ahope1', 'allergy_med', 'test_boost', 'creatine', 'aura', 'drink_indicator', 'food_indicator', 'fitness_indicator', 'vacation', 'prunes', 'odor_indicator', 'meditation_indicator']\n",
      "\n",
      "\n",
      "# Create data frame \"mdata\" holding index and column names\n",
      "mdata = pd.DataFrame(index=index, columns=cols)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "US/Pacific\n",
        "<class 'pandas.tslib.Timestamp'>\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print index[1000]\n",
      "print index[10001]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-03-07 09:40:00-08:00\n",
        "2014-03-13 16:41:00-07:00\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Grab datetime from Google\n",
      "gdate = \"2014-03-03T17:00:00-08:00\"\n",
      "\n",
      "# Convert Google datetime to Pandas Timestamp with US/Pacific timezone\n",
      "gdate_time =  pd.Timestamp(gdate, tz='US/Pacific')\n",
      "\n",
      "print gdate_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-03-03 17:00:00-08:00\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert to pandas timestamp\n",
      "print pd.to_datetime(gdate, utc=True)\n",
      "print pd.to_datetime(gdate) < index[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-03-03 17:00:00-08:00\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Populate data frame with calendar data from Google"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Populate mdata dataframe with Google Calendar Data\n",
      "\n",
      "page_token = None\n",
      "while True:\n",
      "    # Note that for March 2014 and December 2014, offset is -8\n",
      "    # Previously timeMax='2014-12-01T23:59:00-08:00'\n",
      "    events = service.events().list(calendarId=\"bwbelljr@gmail.com\", pageToken=page_token, timeMin='2014-03-06T18:00:00-07:00',timeMax='2014-12-01T23:59:00-08:00').execute()\n",
      "    for event in events['items']:\n",
      "        #print \"----------------\"\n",
      "        #print \"----------------\"\n",
      "\n",
      "        # Check that event starts in my defined time...\n",
      "        if event.has_key('start') and event.has_key('end') and event['start'].has_key('dateTime') and event['end'].has_key('dateTime') and event.has_key('summary'):\n",
      "            gcal_start = str(event['start']['dateTime'])\n",
      "            gcal_start_timestamp =  pd.Timestamp(gcal_start, tz='US/Pacific')\n",
      "            if gcal_start_timestamp > pd.to_datetime(gdate):\n",
      "                \n",
      "                #print gcal_start_timestamp\n",
      "                \n",
      "                gcal_end = str(event['end']['dateTime'])\n",
      "                gcal_end_timestamp = pd.Timestamp(gcal_end, tz='US/Pacific')\n",
      "                #print gcal_end_timestamp\n",
      "\n",
      "                summary = event['summary']\n",
      "                \n",
      "                # Manually remove weird unicode characters\n",
      "                summary=summary.replace(u'\\u201c',u'\\\"')\n",
      "                summary=summary.replace(u'\\u201d',u'\\\"')\n",
      "                summary=summary.replace(u'\\xf1',u'\\\"')\n",
      "                \n",
      "                # Convert to string\n",
      "                gcal_summary = str(summary)\n",
      "                #print gcal_summary\n",
      "\n",
      "                gcal_time_duration = gcal_end_timestamp - gcal_start_timestamp\n",
      "                # I used .seconds because .min did not work.\n",
      "                # Added 1 to delta_min\n",
      "                delta_min = int((gcal_time_duration.seconds / 60) + 1)\n",
      "                # Print delta_min as sanity check\n",
      "                # print delta_min\n",
      "\n",
      "                # Add calendar event descriptions to mdata dataframe\n",
      "                for i in range(delta_min):\n",
      "                    \n",
      "                    #print gcal_start_timestamp + timedelta(minutes=i)\n",
      "                    \n",
      "                    # if calendar_summary1 does not have a string, put summary in calendar_summary1 \n",
      "                    if not isinstance(mdata.calendar_summary1[gcal_start_timestamp + timedelta(minutes=i)], basestring):\n",
      "                        mdata.calendar_summary1[gcal_start_timestamp + timedelta(minutes=i)] = gcal_summary\n",
      "\n",
      "                    # if calendar_summary2 does not have a string, put summary in calendar_summary2\n",
      "                    elif not isinstance(mdata.calendar_summary2[gcal_start_timestamp + timedelta(minutes=i)], basestring):\n",
      "                        mdata.calendar_summary2[gcal_start_timestamp + timedelta(minutes=i)] = gcal_summary\n",
      "\n",
      "                    # if calendar_summary3 does not have a string, put summary in calendar_summary3\n",
      "                    elif not isinstance(mdata.calendar_summary3[gcal_start_timestamp + timedelta(minutes=i)], basestring):\n",
      "                        mdata.calendar_summary3[gcal_start_timestamp + timedelta(minutes=i)] = gcal_summary\n",
      "\n",
      "                    # if all other summary fields are full, put it here...\n",
      "                    else:\n",
      "                        mdata.calendar_summary4[gcal_start_timestamp + timedelta(minutes=i)] = gcal_summary       \n",
      "                    \n",
      "#                    elif not mdata.calendar_summary2[gcal_start_timestamp + timedelta(minutes=i)]:\n",
      "#                        mdata.calendar_summary2[gcal_start_timestamp + timedelta(minutes=i)] = gcal_summary\n",
      "                    \n",
      "#                    elif not mdata.calendar_summary3[gcal_start_timestamp + timedelta(minutes=i)]:\n",
      "#                        mdata.calendar_summary3[gcal_start_timestamp + timedelta(minutes=i)] = gcal_summary\n",
      "        \n",
      "#                    else: \n",
      "#                        mdata.calendar_summary4[gcal_start_timestamp + timedelta(minutes=i)] = gcal_summary\n",
      "    \n",
      "    page_token = events.get('nextPageToken')\n",
      "    if not page_token:\n",
      "        break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Save data frame to CSV file for future reference"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdata.to_csv('migraine_google_calendar_only_Backup.csv', index=True,header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Just in case anything doesn't work out with categorization... We can always reset mdata = mdata_backup\n",
      "# Worst case, just reload from .csv file...\n",
      "mdata_backup = mdata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To revert to fresh google calendar-only dataframe, run the following command!\n",
      "mdata = mdata_backup\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Use Regular expressions to extract features from Google Calendar Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# OPTIMIZED VERSION!\n",
      "# Create Function for regex matching from concatenated calendar events to dataframe columns\n",
      "\n",
      "def regex_match_df(test_string, df_index):\n",
      "\n",
      "    # Initialize all indicator variables to 0\n",
      "    mdata.sickness[df_index] = mdata.flight[df_index] = mdata.medical[df_index] = mdata.dissertation[df_index] = 0 \n",
      "    mdata.odor_indicator[df_index] = 0\n",
      "    mdata.berkeley[df_index] = 0\n",
      "    mdata.barber[df_index] = mdata.teeth[df_index] = mdata.meetup[df_index] = 0 \n",
      "    mdata.class_time[df_index] = 0\n",
      "    mdata.meditation_indicator[df_index] = 0 \n",
      "    mdata.chocolate[df_index] = mdata.soup[df_index] = mdata.dairy[df_index] = 0 \n",
      "    mdata.citrus[df_index] = mdata.gluten[df_index] = mdata.onion[df_index] = 0\n",
      "    mdata.pickles[df_index] = mdata.olives[df_index] = mdata.garlic[df_index] = mdata.beans[df_index] = 0\n",
      "    mdata.banana[df_index] = mdata.avocado[df_index] = mdata.papaya[df_index] = mdata.alcohol[df_index] = 0\n",
      "    mdata.raisin[df_index] = mdata.nuts[df_index] = mdata.pain_med[df_index] = 0\n",
      "    mdata.ausanil[df_index] = mdata.tramadol[df_index] = mdata.antibiotic[df_index] = mdata.hbr[df_index] = 0\n",
      "    mdata.benadryl[df_index] = mdata.neilmed[df_index] = mdata.herbal_med[df_index] =  mdata.flonase[df_index] = 0\n",
      "    mdata.magnesium[df_index] = mdata.pj[df_index] = 0 \n",
      "    mdata.anefrin[df_index] = mdata.acetometophin[df_index] = mdata.supp_med[df_index] = mdata.motrin[df_index] = 0\n",
      "    mdata.ginkgo[df_index] = mdata.aleve[df_index] = mdata.allegra[df_index] = mdata.water[df_index] = 0\n",
      "    mdata.melatonin[df_index] = mdata.htp[df_index] = mdata.fish_oil[df_index] = mdata.niacin[df_index] = 0 \n",
      "    mdata.coq10[df_index] = mdata.zinc[df_index] = mdata.glucosamine[df_index] = mdata.chondroitin[df_index] = 0\n",
      "    mdata.vit_c[df_index] = mdata.vit_d[df_index] = mdata.butterbur[df_index] = mdata.feverfew[df_index] = 0\n",
      "    mdata.dymista[df_index] = mdata.topical_analgesic[df_index] = 0 \n",
      "    mdata.bc_powder[df_index] = mdata.montelukast[df_index] = mdata.zyrtec[df_index] = mdata.multivitamin[df_index] = 0\n",
      "    mdata.riboflavin[df_index] = mdata.chromium[df_index] = mdata.tb[df_index] = mdata.cough_drop[df_index] = 0\n",
      "    mdata.caffeine[df_index] = mdata.loratadine[df_index] = 0\n",
      "    mdata.maxalt[df_index] = mdata.zomig[df_index] = mdata.hedelman2[df_index] = mdata.ahope2[df_index] = 0 \n",
      "    mdata.ahope1[df_index] = mdata.allergy_med[df_index] = mdata.prunes[df_index] = 0\n",
      "    mdata.test_boost[df_index] = mdata.creatine[df_index] = mdata.aura[df_index] = mdata.food_indicator[df_index] = 0 \n",
      "    mdata.drink_indicator[df_index] = mdata.fitness_indicator[df_index] = mdata.vacation[df_index] = 0\n",
      "    \n",
      "    if re.search(\"[w|W]ater\", test_string):\n",
      "        mdata.water[df_index] = 1\n",
      "\n",
      "    if re.search(\"[m|M]elaton\", test_string):\n",
      "        mdata.melatonin[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"HTP\", test_string):\n",
      "        mdata.htp[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[f|F]ish [o|O]il\", test_string):\n",
      "        mdata.fish_oil[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[n|N]iacin\", test_string):\n",
      "        mdata.niacin[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[c|C]o[ ]?[-]?[q|Q][ ]?10\", test_string):\n",
      "        mdata.coq10[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[z|Z]inc\", test_string):\n",
      "        mdata.zinc[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[g|G]lucosamine\", test_string):\n",
      "        mdata.glucosamine[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]hondroitin\", test_string):\n",
      "        mdata.chondroitin[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]itamin [c|C]\", test_string):\n",
      "        mdata.vit_c[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "   \n",
      "    if re.search(\"[b|B]utterbur\", test_string):\n",
      "        mdata.butterbur[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[f|F]everfew\", test_string):\n",
      "        mdata.feverfew[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[d|D]ymista\", test_string):\n",
      "        mdata.dymista[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]aporub\", test_string) or re.search(\"[m|M]edicated [c|C]hest [r|R]ub\", test_string) or re.search(\"[m|M]entholatum [o|O]intment\", test_string) or re.search(\"[a|A]ctivon\", test_string) or re.search(\"[c|C]amphor\", test_string) or re.search(\"[p|P]ain [r|R]elieving\", test_string) or re.search(\"[a|A]veda\", test_string):\n",
      "        mdata.topical_analgesic[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B][c|C] [p|P]owder\", test_string):\n",
      "        mdata.bc_powder[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[m|M]ontelukast\", test_string):\n",
      "        mdata.montelukast[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]etirizine\", test_string):\n",
      "        mdata.zyrtec[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[m|M]ultivitamin\", test_string):\n",
      "        mdata.multivitamin[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]itamin [d|D]3\", test_string):\n",
      "        mdata.vit_d[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]itamin b[-]?2\", test_string) or re.search(\"[r|R]iboflavin\", test_string):\n",
      "        mdata.riboflavin[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]hromium\", test_string):\n",
      "        mdata.chromium[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"TB\", test_string):\n",
      "        mdata.tb[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[l|L]oratadine\", test_string):\n",
      "        mdata.loratadine[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[c|C]ough [d|D]rops\", test_string):\n",
      "        mdata.cough_drop[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[c|C]affeine\", test_string) or re.search(\"[c|C]oke\", test_string) or re.search(\"[c|C]offee\", test_string) or re.search(\"[m|M]onster [e|E]nergy\", test_string) or re.search(\"[r|R]ock[ ]?[s|S]tar\", test_string) or re.search(\"[r|R]ockstar\", test_string) or re.search(\"[b|B]lack [t|T]ea\", test_string) or re.search(\"[g|G]reen [t|T]ea\", test_string) or re.search(\"[s|S]piced [t|T]ea\", test_string) or re.search(\"[p|P]ear [t|T]ea\", test_string) or re.search(\"[m|M]ango [t|T]ea\", test_string) or re.search(\"[r|R]ooibos [t|T]ea\", test_string) or re.search(\"[s|S]picy [t|T]ea\", test_string):\n",
      "        mdata.caffeine[df_index] = 1\n",
      "\n",
      "    if re.search(\"[m|M]axalt\", test_string):\n",
      "        mdata.maxalt[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[z|Z]omig\", test_string):\n",
      "        mdata.zomig[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[h|H]edelman [(]2[)]\", test_string):\n",
      "        mdata.hedelman2[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"AHope[(]2[)]\", test_string) or re.search(\"Ahope[(]1, 2[)]\", test_string) :\n",
      "        mdata.ahope2[df_index] = 1\n",
      "\n",
      "    if re.search(\"Ahope[(]1[])]\", test_string) or re.search(\"Ahope[(]1, 2[)]\", test_string) :\n",
      "        mdata.ahope1[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]llegra\", test_string):\n",
      "        mdata.allegra[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[n|N]aproxen\", test_string):\n",
      "        mdata.aleve[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[g|G]inkgo\", test_string):\n",
      "        mdata.ginkgo[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[m|M]otrin\", test_string):\n",
      "        mdata.motrin[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]ooster\", test_string):\n",
      "        mdata.test_boost[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "        mdata.ginkgo[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]cetometophin\", test_string) or re.search(\"cold and cough\", test_string) or re.search(\"cold & cough\", test_string):\n",
      "        mdata.acetometophin[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[a|A]nefrin\", test_string):\n",
      "        mdata.anefrin[df_index] = 1\n",
      "    \n",
      "    if re.search(\"PJ\", test_string):\n",
      "        mdata.pj[df_index] = 1\n",
      "\n",
      "    if re.search(\"[f|F]luticasone\", test_string):\n",
      "        mdata.flonase[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[m|M]agnesium\", test_string):\n",
      "        mdata.magnesium[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[n|N]eilmed\", test_string):\n",
      "        mdata.neilmed[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[d|D]iphenhydramine\", test_string):\n",
      "        mdata.benadryl[df_index] = 1\n",
      "\n",
      "    if re.search(\"cold and cough\", test_string) or re.search(\"cold & cough\", test_string):\n",
      "        mdata.hbr[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]zithromycin\", test_string):\n",
      "        mdata.antibiotic[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[t|T]ramadol\", test_string):\n",
      "        mdata.tramadol[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]usanil\", test_string):\n",
      "        mdata.ausanil[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[n|N]ut\", test_string) or re.search(\"[a|A]lmond\", test_string) or re.search(\"[p|P]eanut\", test_string) or re.search(\"[c|C]ashew\", test_string) or re.search(\"[p|P]ecan\", test_string) or re.search(\"[m|M]acadamia\", test_string) or re.search(\"[p|P]istachio\", test_string) or re.search(\"[w|W]alnut\", test_string):\n",
      "        mdata.nuts[df_index] = 1\n",
      "\n",
      "    if re.search(\"[r|R]aisin\", test_string):\n",
      "        mdata.raisin[df_index] = 1\n",
      "\n",
      "    # for some reason i don't presently understand,     \n",
      "    if re.search(\"[w|W]ine\", test_string) or re.search(\"[z|Z]acapa\", test_string) or re.search(\"Mark\", test_string) or re.search(\"[v|V]odka\", test_string) or re.search(\"[w|W]oodford\", test_string) or re.search(\"[b|B]eer\", test_string) or re.search(\"[m|M]ichelob\", test_string) or re.search(\"[m|M]argarita\", test_string) or re.search(\"[k|K]ona\", test_string) or re.search(\"[q|Q]uasar\", test_string) or re.search(\"[b|B]lue [m|M]oon\", test_string) or re.search(\"[b|B]elgian [a|A]le\", test_string) or re.search(\"[g|G]inger [a|A]le\", test_string) or re.search(\"[w|W]heaten [a|A]le\", test_string) or re.search(\"[p|P]inot\", test_string) or re.search(\"[c|C]hardonnay\", test_string) or re.search(\"[c|C]hampagne\", test_string) or re.search(\"[m|M]erlot\", test_string) or re.search(\"[s|S]auvignon\", test_string) or re.search(\"[b|B]ourbon\", test_string):\n",
      "        mdata.alcohol[df_index] = 1\n",
      "\n",
      "    if re.search(\"[p|P]apaya\", test_string):\n",
      "        mdata.papaya[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]vocado\", test_string) or re.search(\"[g|G]uacamole\", test_string):\n",
      "        mdata.avocado[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]anana\", test_string):\n",
      "        mdata.banana[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]lack [b|B]ean\", test_string) or re.search(\"[g|G]arbanzo [b|B]ean\", test_string) or re.search(\"[l|L]ima [b|B]ean\", test_string) or re.search(\"[k|K]idney [b|B]ean\", test_string) or re.search(\"[p|P]into [b|B]ean\", test_string) or re.search(\"[b|B]ean [b|B]urrito\", test_string) or re.search(\"[b|B]ean [p|P]att\", test_string):\n",
      "        mdata.beans[df_index] = 1\n",
      "\n",
      "    if re.search(\"[g|G]arlic\", test_string):\n",
      "        mdata.garlic[df_index] = 1\n",
      "\n",
      "    if re.search(\"[o|O]lives\", test_string):\n",
      "        mdata.olives[df_index] = 1\n",
      "\n",
      "    if re.search(\"[p|P]ickle\", test_string):\n",
      "        mdata.pickles[df_index] = 1\n",
      "\n",
      "    if re.search(\"[o|O]nion\", test_string):\n",
      "        mdata.onion[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]read\", test_string) or re.search(\"[p|P]izza\", test_string) or re.search(\"[c|C]ake\", test_string) or re.search(\"[p|P]ie\", test_string) or re.search(\"[c|C]roissant\", test_string) or re.search(\"[c|C]ookie\", test_string) or re.search(\"[c|C]hocolate [c|C]hip\", test_string) or re.search(\"[c|C]routons\", test_string) or re.search(\"[p|P]asta\", test_string) or re.search(\"fried chicken\", test_string) or re.search(\"fried okra\", test_string) or re.search(\"[m|M]ein\", test_string) or re.search(\"[p|P]ancake\", test_string) or re.search(\"[w|W]heat\", test_string) or re.search(\"[d|D]oritos\", test_string) or re.search(\"[c|C]heeto\", test_string) or re.search(\"[b|B]un\", test_string) or re.search(\"[m|M]uffin\", test_string) or re.search(\"[t|T]oast\", test_string) or re.search(\"[w|W]affle\", test_string) or re.search(\"[s|S]andwich\", test_string) or re.search(\"[b|B]urger\", test_string) or re.search(\"[b|B]eer\", test_string):\n",
      "        mdata.gluten[df_index] = 1\n",
      "\n",
      "    if re.search(\"[o|O]range\", test_string) or re.search(\", [l|L]emon\", test_string) or re.search(\", [l|L]ime\", test_string) or re.search(\"fresh lime\", test_string):\n",
      "        mdata.citrus[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]heese\", test_string) or re.search(\"[m|M]ozzarella\", test_string) or re.search(\"[p|P]armesan\", test_string) or re.search(\"[p|P]rovolone\", test_string) or re.search(\"[c|C]heddar\", test_string) or re.search(\"[s|S]our [c|C]ream\", test_string) or re.search(\", [m|M]ilk\", test_string) or re.search(\"vanilla ice\", test_string) or re.search(\"crunch ice\", test_string):\n",
      "        mdata.dairy[df_index] = 1\n",
      "\n",
      "    # Will skip processed meat for now. Need to inspect ingredients for processed/aged/cured meats as well as nitrates/nitrites.\n",
      "\n",
      "    if re.search(\"[s|S]oup\", test_string):\n",
      "        mdata.soup[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]hocolate\", test_string):\n",
      "        mdata.chocolate[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]reatine\", test_string):\n",
      "        mdata.creatine[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[b|B]erkeley:\", test_string):\n",
      "        mdata.berkeley[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[c|C]lass:\", test_string):\n",
      "        mdata.class_time[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[m|M]edical:\", test_string) or re.search(\"[h|H]ealth:\", test_string):\n",
      "        mdata.medical[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[m|M]eetup:\", test_string):\n",
      "        mdata.meetup[df_index] = 1\n",
      "\n",
      "    if re.search(\"[f|F]light:\", test_string):\n",
      "        mdata.flight[df_index] = 1\n",
      "\n",
      "    if re.search(\"[s|S]ickness:\", test_string):\n",
      "        mdata.sickness[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]isual:\", test_string):\n",
      "        mdata.aura[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[p|P]runes\", test_string):\n",
      "        mdata.prunes[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[v|V]acation:\", test_string):\n",
      "        mdata.vacation[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]arber\", test_string):\n",
      "        mdata.barber[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[d|D]rink:\", test_string):\n",
      "        mdata.drink_indicator[df_index] = 1\n",
      "        if re.search(\"[w|W]ater\", test_string):\n",
      "        mdata.water[df_index] = 1\n",
      "\n",
      "    if re.search(\"[m|M]elaton\", test_string):\n",
      "        mdata.melatonin[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"HTP\", test_string):\n",
      "        mdata.htp[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[f|F]ish [o|O]il\", test_string):\n",
      "        mdata.fish_oil[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[n|N]iacin\", test_string):\n",
      "        mdata.niacin[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[c|C]o[ ]?[-]?[q|Q][ ]?10\", test_string):\n",
      "        mdata.coq10[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[z|Z]inc\", test_string):\n",
      "        mdata.zinc[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[g|G]lucosamine\", test_string):\n",
      "        mdata.glucosamine[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]hondroitin\", test_string):\n",
      "        mdata.chondroitin[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]itamin [c|C]\", test_string):\n",
      "        mdata.vit_c[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "   \n",
      "    if re.search(\"[b|B]utterbur\", test_string):\n",
      "        mdata.butterbur[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[f|F]everfew\", test_string):\n",
      "        mdata.feverfew[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[d|D]ymista\", test_string):\n",
      "        mdata.dymista[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]aporub\", test_string) or re.search(\"[m|M]edicated [c|C]hest [r|R]ub\", test_string) or re.search(\"[m|M]entholatum [o|O]intment\", test_string) or re.search(\"[a|A]ctivon\", test_string) or re.search(\"[c|C]amphor\", test_string) or re.search(\"[p|P]ain [r|R]elieving\", test_string) or re.search(\"[a|A]veda\", test_string):\n",
      "        mdata.topical_analgesic[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B][c|C] [p|P]owder\", test_string):\n",
      "        mdata.bc_powder[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[m|M]ontelukast\", test_string):\n",
      "        mdata.montelukast[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]etirizine\", test_string):\n",
      "        mdata.zyrtec[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[m|M]ultivitamin\", test_string):\n",
      "        mdata.multivitamin[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]itamin [d|D]3\", test_string):\n",
      "        mdata.vit_d[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]itamin b[-]?2\", test_string) or re.search(\"[r|R]iboflavin\", test_string):\n",
      "        mdata.riboflavin[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]hromium\", test_string):\n",
      "        mdata.chromium[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"TB\", test_string):\n",
      "        mdata.tb[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[l|L]oratadine\", test_string):\n",
      "        mdata.loratadine[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[c|C]ough [d|D]rops\", test_string):\n",
      "        mdata.cough_drop[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[c|C]affeine\", test_string) or re.search(\"[c|C]oke\", test_string) or re.search(\"[c|C]offee\", test_string) or re.search(\"[m|M]onster [e|E]nergy\", test_string) or re.search(\"[r|R]ock[ ]?[s|S]tar\", test_string) or re.search(\"[r|R]ockstar\", test_string) or re.search(\"[b|B]lack [t|T]ea\", test_string) or re.search(\"[g|G]reen [t|T]ea\", test_string) or re.search(\"[s|S]piced [t|T]ea\", test_string) or re.search(\"[p|P]ear [t|T]ea\", test_string) or re.search(\"[m|M]ango [t|T]ea\", test_string) or re.search(\"[r|R]ooibos [t|T]ea\", test_string) or re.search(\"[s|S]picy [t|T]ea\", test_string):\n",
      "        mdata.caffeine[df_index] = 1\n",
      "\n",
      "    if re.search(\"[m|M]axalt\", test_string):\n",
      "        mdata.maxalt[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[z|Z]omig\", test_string):\n",
      "        mdata.zomig[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[h|H]edelman [(]2[)]\", test_string):\n",
      "        mdata.hedelman2[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"AHope[(]2[)]\", test_string) or re.search(\"Ahope[(]1, 2[)]\", test_string) :\n",
      "        mdata.ahope2[df_index] = 1\n",
      "\n",
      "    if re.search(\"Ahope[(]1[])]\", test_string) or re.search(\"Ahope[(]1, 2[)]\", test_string) :\n",
      "        mdata.ahope1[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]llegra\", test_string):\n",
      "        mdata.allegra[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[n|N]aproxen\", test_string):\n",
      "        mdata.aleve[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[g|G]inkgo\", test_string):\n",
      "        mdata.ginkgo[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[m|M]otrin\", test_string):\n",
      "        mdata.motrin[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]ooster\", test_string):\n",
      "        mdata.test_boost[df_index] = 1\n",
      "        mdata.supp_med[df_index] = 1\n",
      "        mdata.ginkgo[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]cetometophin\", test_string) or re.search(\"cold and cough\", test_string) or re.search(\"cold & cough\", test_string):\n",
      "        mdata.acetometophin[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[a|A]nefrin\", test_string):\n",
      "        mdata.anefrin[df_index] = 1\n",
      "    \n",
      "    if re.search(\"PJ\", test_string):\n",
      "        mdata.pj[df_index] = 1\n",
      "\n",
      "    if re.search(\"[f|F]luticasone\", test_string):\n",
      "        mdata.flonase[df_index] = 1\n",
      "        mdata.allergy_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[m|M]agnesium\", test_string):\n",
      "        mdata.magnesium[df_index] = 1\n",
      "        mdata.herbal_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[n|N]eilmed\", test_string):\n",
      "        mdata.neilmed[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[d|D]iphenhydramine\", test_string):\n",
      "        mdata.benadryl[df_index] = 1\n",
      "\n",
      "    if re.search(\"cold and cough\", test_string) or re.search(\"cold & cough\", test_string):\n",
      "        mdata.hbr[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]zithromycin\", test_string):\n",
      "        mdata.antibiotic[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[t|T]ramadol\", test_string):\n",
      "        mdata.tramadol[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]usanil\", test_string):\n",
      "        mdata.ausanil[df_index] = 1\n",
      "        mdata.pain_med[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[n|N]ut\", test_string) or re.search(\"[a|A]lmond\", test_string) or re.search(\"[p|P]eanut\", test_string) or re.search(\"[c|C]ashew\", test_string) or re.search(\"[p|P]ecan\", test_string) or re.search(\"[m|M]acadamia\", test_string) or re.search(\"[p|P]istachio\", test_string) or re.search(\"[w|W]alnut\", test_string):\n",
      "        mdata.nuts[df_index] = 1\n",
      "\n",
      "    if re.search(\"[r|R]aisin\", test_string):\n",
      "        mdata.raisin[df_index] = 1\n",
      "\n",
      "    # for some reason i don't presently understand,     \n",
      "    if re.search(\"[w|W]ine\", test_string) or re.search(\"[z|Z]acapa\", test_string) or re.search(\"Mark\", test_string) or re.search(\"[v|V]odka\", test_string) or re.search(\"[w|W]oodford\", test_string) or re.search(\"[b|B]eer\", test_string) or re.search(\"[m|M]ichelob\", test_string) or re.search(\"[m|M]argarita\", test_string) or re.search(\"[k|K]ona\", test_string) or re.search(\"[q|Q]uasar\", test_string) or re.search(\"[b|B]lue [m|M]oon\", test_string) or re.search(\"[b|B]elgian [a|A]le\", test_string) or re.search(\"[g|G]inger [a|A]le\", test_string) or re.search(\"[w|W]heaten [a|A]le\", test_string) or re.search(\"[p|P]inot\", test_string) or re.search(\"[c|C]hardonnay\", test_string) or re.search(\"[c|C]hampagne\", test_string) or re.search(\"[m|M]erlot\", test_string) or re.search(\"[s|S]auvignon\", test_string) or re.search(\"[b|B]ourbon\", test_string):\n",
      "        mdata.alcohol[df_index] = 1\n",
      "\n",
      "    if re.search(\"[p|P]apaya\", test_string):\n",
      "        mdata.papaya[df_index] = 1\n",
      "\n",
      "    if re.search(\"[a|A]vocado\", test_string) or re.search(\"[g|G]uacamole\", test_string):\n",
      "        mdata.avocado[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]anana\", test_string):\n",
      "        mdata.banana[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]lack [b|B]ean\", test_string) or re.search(\"[g|G]arbanzo [b|B]ean\", test_string) or re.search(\"[l|L]ima [b|B]ean\", test_string) or re.search(\"[k|K]idney [b|B]ean\", test_string) or re.search(\"[p|P]into [b|B]ean\", test_string) or re.search(\"[b|B]ean [b|B]urrito\", test_string) or re.search(\"[b|B]ean [p|P]att\", test_string):\n",
      "        mdata.beans[df_index] = 1\n",
      "\n",
      "    if re.search(\"[g|G]arlic\", test_string):\n",
      "        mdata.garlic[df_index] = 1\n",
      "\n",
      "    if re.search(\"[o|O]lives\", test_string):\n",
      "        mdata.olives[df_index] = 1\n",
      "\n",
      "    if re.search(\"[p|P]ickle\", test_string):\n",
      "        mdata.pickles[df_index] = 1\n",
      "\n",
      "    if re.search(\"[o|O]nion\", test_string):\n",
      "        mdata.onion[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]read\", test_string) or re.search(\"[p|P]izza\", test_string) or re.search(\"[c|C]ake\", test_string) or re.search(\"[p|P]ie\", test_string) or re.search(\"[c|C]roissant\", test_string) or re.search(\"[c|C]ookie\", test_string) or re.search(\"[c|C]hocolate [c|C]hip\", test_string) or re.search(\"[c|C]routons\", test_string) or re.search(\"[p|P]asta\", test_string) or re.search(\"fried chicken\", test_string) or re.search(\"fried okra\", test_string) or re.search(\"[m|M]ein\", test_string) or re.search(\"[p|P]ancake\", test_string) or re.search(\"[w|W]heat\", test_string) or re.search(\"[d|D]oritos\", test_string) or re.search(\"[c|C]heeto\", test_string) or re.search(\"[b|B]un\", test_string) or re.search(\"[m|M]uffin\", test_string) or re.search(\"[t|T]oast\", test_string) or re.search(\"[w|W]affle\", test_string) or re.search(\"[s|S]andwich\", test_string) or re.search(\"[b|B]urger\", test_string) or re.search(\"[b|B]eer\", test_string):\n",
      "        mdata.gluten[df_index] = 1\n",
      "\n",
      "    if re.search(\"[o|O]range\", test_string) or re.search(\", [l|L]emon\", test_string) or re.search(\", [l|L]ime\", test_string) or re.search(\"fresh lime\", test_string):\n",
      "        mdata.citrus[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]heese\", test_string) or re.search(\"[m|M]ozzarella\", test_string) or re.search(\"[p|P]armesan\", test_string) or re.search(\"[p|P]rovolone\", test_string) or re.search(\"[c|C]heddar\", test_string) or re.search(\"[s|S]our [c|C]ream\", test_string) or re.search(\", [m|M]ilk\", test_string) or re.search(\"vanilla ice\", test_string) or re.search(\"crunch ice\", test_string):\n",
      "        mdata.dairy[df_index] = 1\n",
      "\n",
      "    # Will skip processed meat for now. Need to inspect ingredients for processed/aged/cured meats as well as nitrates/nitrites.\n",
      "\n",
      "    if re.search(\"[s|S]oup\", test_string):\n",
      "        mdata.soup[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]hocolate\", test_string):\n",
      "        mdata.chocolate[df_index] = 1\n",
      "\n",
      "    if re.search(\"[c|C]reatine\", test_string):\n",
      "        mdata.creatine[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[b|B]erkeley:\", test_string):\n",
      "        mdata.berkeley[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[c|C]lass:\", test_string):\n",
      "        mdata.class_time[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[m|M]edical:\", test_string) or re.search(\"[h|H]ealth:\", test_string):\n",
      "        mdata.medical[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[m|M]eetup:\", test_string):\n",
      "        mdata.meetup[df_index] = 1\n",
      "\n",
      "    if re.search(\"[f|F]light:\", test_string):\n",
      "        mdata.flight[df_index] = 1\n",
      "\n",
      "    if re.search(\"[s|S]ickness:\", test_string):\n",
      "        mdata.sickness[df_index] = 1\n",
      "\n",
      "    if re.search(\"[v|V]isual:\", test_string):\n",
      "        mdata.aura[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[p|P]runes\", test_string):\n",
      "        mdata.prunes[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[v|V]acation:\", test_string):\n",
      "        mdata.vacation[df_index] = 1\n",
      "\n",
      "    if re.search(\"[b|B]arber\", test_string):\n",
      "        mdata.barber[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[d|D]rink:\", test_string):\n",
      "        mdata.drink_indicator[df_index] = 1\n",
      "        \n",
      "    if re.search(\"[f|F]ood:\", test_string):\n",
      "        mdata.food_indicator[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[f|F]itness:\", test_string):\n",
      "        mdata.fitness_indicator[df_index] = 1\n",
      "    if re.search(\"[f|F]ood:\", test_string):\n",
      "        mdata.food_indicator[df_index] = 1\n",
      "    \n",
      "    if re.search(\"[f|F]itness:\", test_string):\n",
      "        mdata.fitness_indicator[df_index] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test function regex_match_df\n",
      "# regex_match_df(\"Drink: coke zero, water (24 oz), Ron zacapa rum, rockstar energy\", 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Start here... Perform operations on the dataframe... say the first 10000 or so. Explain issues with Hassan. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import timeit\n",
      "\n",
      "# 400 records is 35.9826516553 seconds\n",
      "# 4000 records is 347.708792775... Less than 10-fold difference!\n",
      "\n",
      "# 400 records with changed concat is 33.113998438 seconds...\n",
      "# 4000 records with changed concat is 354.232949723 seconds... and 380.512785561\n",
      "\n",
      "# xrange AND 4000 records with changed concat is 368.892346505 seconds...\n",
      "\n",
      "# xrange AND 4000 records is 398.277183229 seconds...\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_time = timeit.default_timer()\n",
      "\n",
      "# Here, we are splitting up Google Calendar events into respective columns\n",
      "# Turn these into functions/methods... The repetition is problematic...\n",
      "\n",
      "# Range (400000) spans data range from March 6, 2014 to Dec 9, 2014\n",
      "for i in range(400000):\n",
      "    \n",
      "    # Explain has_category\n",
      "    has_category = 0\n",
      "    \n",
      "    # create index variable to easily access rows of dataframe\n",
      "    mdata_index = mdata.index[i]\n",
      "    \n",
      "    # Save respective calendar events into variables names summaryX\n",
      "    summary1 = str(mdata.calendar_summary1[mdata_index])\n",
      "    summary2 = str(mdata.calendar_summary2[mdata_index])\n",
      "    summary3 = str(mdata.calendar_summary3[mdata_index])\n",
      "    summary4 = str(mdata.calendar_summary4[mdata_index])\n",
      "    \n",
      "    # Note that an empty cell is \"nan.\" So 4 empty entries would appear as \"nannannannan\"\n",
      "    # This should not be a problem for the regular expression extraction process\n",
      "    # concat_summary = \"%s%s%s%s\" % (summary1, summary2, summary3, summary4)\n",
      "    concat_summary = summary1 + summary2 + summary3 + summary4\n",
      "    \n",
      "    # Call Regex matching function here\n",
      "    regex_match_df(concat_summary, mdata_index)\n",
      "    \n",
      "    existing_drink = mdata.drink[mdata_index]\n",
      "    drink_content = \"\"\n",
      "    \n",
      "    # Extract Drink Content\n",
      "    #if re.search(\"\\s*[d|D]rink\\s*:\", summary1):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Drink Content\n",
      "    #    drink_content = re.split('\\s*:\\s*', summary1)[1]\n",
      "    #    # Consider additional code to extract quantity consumed...\n",
      "    #    \n",
      "    #    # Check to see if we already have drink content. \n",
      "    #    # If so, concatenate to this string...\n",
      "    #    # Change this code in next version!\n",
      "    #    if math.isnan(mdata.drink[mdata_index]):\n",
      "    #        mdata.drink[mdata_index] = drink_content\n",
      "    #    else:\n",
      "    #        old_drink_data = str(existing_drink)\n",
      "    #        mdata.drink[mdata_index] = old_drink_data + \" , \" + drink_content\n",
      "    \n",
      "    #if re.search(\"\\s*[d|D]rink\\s*:\", summary2):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Drink Content\n",
      "    #    drink_content = re.split('\\s*:\\s*', summary2)[1]\n",
      "    #    #print \"new drink content\" + drink_content\n",
      "    #    # Consider additional code to extract quantity consumed...\n",
      "    #            \n",
      "    #    # Check to see if we already have drink content. \n",
      "    #    # If so, concatenate to this string...\n",
      "    #    if math.isnan(mdata.drink[mdata_index]):\n",
      "    #        mdata.drink[mdata_index] = drink_content\n",
      "    #    else:\n",
      "    #        old_drink_data = str(existing_drink)\n",
      "    #        mdata.drink[mdata_index] = old_drink_data + \" , \" + drink_content\n",
      "    #\n",
      "    #if re.search(\"\\s*[d|D]rink\\s*:\", summary3):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Drink Content\n",
      "    #    drink_content = re.split('\\s*:\\s*', summary3)[1]\n",
      "    #    # Consider additional code to extract quantity consumed...\n",
      "    #    \n",
      "    #    # Check to see if we already have drink content. \n",
      "    #    # If so, concatenate to this string...\n",
      "    #    if math.isnan(mdata.drink[mdata_index]):\n",
      "    #        mdata.drink[mdata_index] = drink_content\n",
      "    #    else:\n",
      "    #        old_drink_data = str(existing_drink)\n",
      "    #        mdata.drink[mdata_index] = old_drink_data + \" , \" + drink_content\n",
      "\n",
      "    #if re.search(\"\\s*[d|D]rink\\s*:\", summary4):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Drink Content\n",
      "    #    drink_content = re.split('\\s*:\\s*', summary4)[1]\n",
      "    #    # Consider additional code to extract quantity consumed...\n",
      "    #\n",
      "    #    # Check to see if we already have drink content. \n",
      "    #    # If so, concatenate to this string...\n",
      "    #    if math.isnan(mdata.drink[mdata_index]):\n",
      "    #        mdata.drink[mdata_index] = drink_content\n",
      "    #    else:\n",
      "    #        old_drink_data = str(existing_drink)\n",
      "    #        mdata.drink[mdata_index] = old_drink_data + \" , \" + drink_content\n",
      "        \n",
      "    # Extract Food Content\n",
      "    #if re.search(\"\\s*[f|F]ood\\s*:\", summary1):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Food Content\n",
      "    #    food_content = re.split('\\s*:\\s*', summary1)[1]\n",
      "    #    # Consider additional code to extract quantity consumed...\n",
      "\n",
      "    #    # Check to see if we already have food content. \n",
      "    #    # If so, concatenate to this string...\n",
      "    #    if not isinstance(mdata.food[mdata_index], basestring):\n",
      "    #        mdata.food[mdata_index] = food_content\n",
      "    #    else:\n",
      "    #        mdata.food[mdata_index] += \" , \" + food_content    \n",
      "\n",
      "    #if re.search(\"\\s*[f|F]food\\s*:\", summary2):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Food Content\n",
      "    #    food_content = re.split('\\s*:\\s*', summary2)[1]\n",
      "    #    # Consider additional code to extract quantity consumed...\n",
      "\n",
      "    #    # Check to see if we already have food content. \n",
      "    #    # If so, concatenate to this string...\n",
      "    #    if not isinstance(mdata.food[mdata_index], basestring):\n",
      "    #        mdata.food[mdata_index] = food_content\n",
      "    #    else:\n",
      "    #        mdata.food[mdata_index] += \" , \" + food_content    \n",
      "            \n",
      "    #if re.search(\"\\s*[f|F]ood\\s*:\", summary3):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Food Content\n",
      "    #    food_content = re.split('\\s*:\\s*', summary3)[1]\n",
      "    #    # Consider additional code to extract quantity consumed...\n",
      "\n",
      "    #    # Check to see if we already have food content. \n",
      "    #    # If so, concatenate to this string...\n",
      "    #    if not isinstance(mdata.food[mdata_index], basestring):\n",
      "    #        mdata.food[mdata_index] = food_content\n",
      "    #    else:\n",
      "    #        mdata.food[mdata_index] += \" , \" + food_content            \n",
      "    \n",
      "    #if re.search(\"\\s*[f|F]ood\\s*:\", summary4):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Food Content\n",
      "    #    food_content = re.split('\\s*:\\s*', summary4)[1]\n",
      "    #    # Consider additional code to extract quantity consumed...\n",
      "\n",
      "    #    # Check to see if we already have food content. \n",
      "    #    # If so, concatenate to this string...\n",
      "    #    if not isinstance(mdata.food[mdata_index], basestring):\n",
      "    #        mdata.food[mdata_index] = food_content\n",
      "    #    else:\n",
      "    #        mdata.food[mdata_index] += \" , \" + food_content    \n",
      "\n",
      "    # Extract KT Content\n",
      "    if re.search(\"KT:\", summary1):\n",
      "        has_category = 1\n",
      "        # Extract KT Content\n",
      "        kt_content = re.split(': ', summary1)[1]\n",
      "        mdata.kt[mdata_index] = kt_content\n",
      "    if re.search(\"KT:\", summary2):\n",
      "        has_category = 1\n",
      "        # Extract KT Content\n",
      "        kt_content = re.split(': ', summary2)[1]\n",
      "        mdata.kt[mdata_index] = kt_content\n",
      "    if re.search(\"KT:\", summary3):\n",
      "        has_category = 1\n",
      "        # Extract KT Content\n",
      "        kt_content = re.split(': ', summary3)[1]\n",
      "        mdata.kt[mdata_index] = kt_content\n",
      "    if re.search(\"KT:\", summary4):\n",
      "        has_category = 1\n",
      "        # Extract KT Content\n",
      "        kt_content = re.split(': ', summary4)[1]\n",
      "        mdata.kt[mdata_index] = kt_content\n",
      "\n",
      "    # Extract Medicine Content\n",
      "    #if re.search(\"\\s*[m|M]edicine\\s*:\", summary1):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Medicine Content\n",
      "    #    medicine_content = re.split('\\s*:\\s*', summary1)[1]\n",
      "        # Consider additional code to extract quantity consumed...\n",
      "\n",
      "        # Check to see if we already have medicine content. \n",
      "        # If so, concatenate to this string...\n",
      "    #    if not isinstance(mdata.medicine[mdata_index], basestring):\n",
      "    #        mdata.medicine[mdata_index] = medicine_content\n",
      "    #    else:\n",
      "    #        mdata.medicine[mdata_index] += \" , \" + medicine_content    \n",
      "    \n",
      "    #    if re.search(\"\\s*[m|M]edicine\\s*:\", summary2):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Medicine Content\n",
      "    #    medicine_content = re.split('\\s*:\\s*', summary2)[1]\n",
      "        # Consider additional code to extract quantity consumed...\n",
      "\n",
      "        # Check to see if we already have medicine content. \n",
      "        # If so, concatenate to this string...\n",
      "    #    if not isinstance(mdata.medicine[mdata_index], basestring):\n",
      "    #        mdata.medicine[mdata_index] = medicine_content\n",
      "    #    else:\n",
      "    #        mdata.medicine[mdata_index] += \" , \" + medicine_content  \n",
      "\n",
      "    #if re.search(\"\\s*[m|M]edicine\\s*:\", summary3):\n",
      "    #    has_category = 1\n",
      "        # Extract Medicine Content\n",
      "    #    medicine_content = re.split('\\s*:\\s*', summary3)[1]\n",
      "        # Consider additional code to extract quantity consumed...\n",
      "\n",
      "        # Check to see if we already have medicine content. \n",
      "        # If so, concatenate to this string...\n",
      "    #    if not isinstance(mdata.medicine[mdata_index], basestring):\n",
      "    #        mdata.medicine[mdata_index] = medicine_content\n",
      "    #    else:\n",
      "    #        mdata.medicine[mdata_index] += \" , \" + medicine_content  \n",
      "        \n",
      "    #if re.search(\"\\s*[m|M]edicine\\s*:\", summary4):\n",
      "    #    has_category = 1\n",
      "        # Extract Medicine Content\n",
      "    #    medicine_content = re.split('\\s*:\\s*', summary4)[1]\n",
      "        # Consider additional code to extract quantity consumed...\n",
      "\n",
      "        # Check to see if we already have medicine content. \n",
      "        # If so, concatenate to this string...\n",
      "    #    if not isinstance(mdata.medicine[mdata_index], basestring):\n",
      "    #        mdata.medicine[mdata_index] = medicine_content\n",
      "    #    else:\n",
      "    #        mdata.medicine[mdata_index] += \" , \" + medicine_content  \n",
      "\n",
      "    # Extract Relaxation Content\n",
      "    #if re.search(\"\\s*[r|R]elaxation\\s*:\", summary1):\n",
      "    #    has_category = 1\n",
      "        # Extract Relaxation Content\n",
      "    #    relaxation_content = re.split('\\s*:\\s*', summary1)[1]\n",
      "    #    mdata.relaxation[mdata_index] = relaxation_content\n",
      "    #if re.search(\"\\s*[r|R]elaxation\\s*:\", summary2):\n",
      "    #    has_category = 1\n",
      "        # Extract Relaxation Content\n",
      "    #    relaxation_content = re.split('\\s*:\\s*', summary2)[1]\n",
      "    #    mdata.relaxation[mdata_index] = relaxation_content\n",
      "    #if re.search(\"\\s*[r|R]elaxation\\s*:\", summary3):\n",
      "    #    has_category = 1\n",
      "        # Extract Relaxation Content\n",
      "    #    relaxation_content = re.split('\\s*:\\s*', summary3)[1]\n",
      "    #    mdata.relaxation[mdata_index] = relaxation_content\n",
      "    #if re.search(\"\\s*[r|R]elaxation\\s*:\", summary4):\n",
      "    #    has_category = 1\n",
      "        # Extract Relaxation Content\n",
      "    #    relaxation_content = re.split('\\s*:\\s*', summary4)[1]\n",
      "    #    mdata.relaxation[mdata_index] = relaxation_content\n",
      "\n",
      "    # Extract Meditation Content\n",
      "    #if re.search(\"\\s*[m|M]editation\\s*:\", summary1):\n",
      "    #    has_category = 1\n",
      "        # Extract Meditation Content\n",
      "    #    meditation_content = re.split('\\s*:\\s*', summary1)[1]\n",
      "    #    mdata.meditation[mdata_index] = meditation_content\n",
      "    #if re.search(\"\\s*[m|M]editation\\s*:\", summary2):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Meditation Content\n",
      "    #    meditation_content = re.split('\\s*:\\s*', summary2)[1]\n",
      "    #    mdata.meditation[mdata_index] = meditation_content\n",
      "    #if re.search(\"\\s*[m|M]editation\\s*:\", summary3):\n",
      "    #    has_category = 1\n",
      "        # Extract Meditation Content\n",
      "    #    meditation_content = re.split('\\s*:\\s*', summary3)[1]\n",
      "    #    mdata.meditation[mdata_index] = meditation_content\n",
      "    #if re.search(\"\\s*[m|M]editation\\s*:\", summary4):\n",
      "    #    has_category = 1\n",
      "        # Extract Meditation Content\n",
      "    #    meditation_content = re.split('\\s*:\\s*', summary4)[1]\n",
      "    #    mdata.meditation[mdata_index] = meditation_content\n",
      "\n",
      "    # Extract Eye Contacts Content\n",
      "    if re.search(\"Eye Contacts:\", summary1):\n",
      "        has_category = 1\n",
      "        # Extract Eye Contacts Content\n",
      "        eye_contacts_content = re.split(': ', summary1)[1]\n",
      "        mdata.eye_contacts[mdata_index] = eye_contacts_content\n",
      "    if re.search(\"Eye Contacts:\", summary2):\n",
      "        has_category = 1\n",
      "        # Extract Eye Contacts Content\n",
      "        eye_contacts_content = re.split(': ', summary2)[1]\n",
      "        mdata.eye_contacts[mdata_index] = eye_contacts_content\n",
      "    if re.search(\"Eye Contacts:\", summary3):\n",
      "        has_category = 1\n",
      "        # Extract Eye Contacts Content\n",
      "        eye_contacts_content = re.split(': ', summary3)[1]\n",
      "        mdata.eye_contacts[mdata_index] = eye_contacts_content\n",
      "    if re.search(\"Eye Contacts:\", summary4):\n",
      "        has_category = 1\n",
      "       # Extract Eye Contacts Content\n",
      "        eye_contacts_content = re.split(': ', summary4)[1]\n",
      "        mdata.eye_contacts[mdata_index] = eye_contacts_content\n",
      "\n",
      "    # Extract Fitness Content\n",
      "    #if re.search(\"\\s*[f|F]itness\\s*:\", summary1):\n",
      "    #    has_category = 1\n",
      "        # Extract Fitness Content\n",
      "    #    fitness_content = re.split('\\s*:\\s*', summary1)[1]\n",
      "    #    mdata.fitness[mdata_index] = fitness_content\n",
      "    #if re.search(\"\\s*[f|F]itness\\s*:\", summary2):\n",
      "    #    has_category = 1\n",
      "        # Extract Fitness Content\n",
      "    #    fitness_content = re.split('\\s*:\\s*', summary2)[1]\n",
      "    #    mdata.fitness[mdata_index] = fitness_content\n",
      "    #if re.search(\"\\s*[f|F]itness\\s*:\", summary3):\n",
      "    #    has_category = 1\n",
      "        # Extract Fitness Content\n",
      "    #    fitness_content = re.split('\\s*:\\s*', summary3)[1]\n",
      "    #    mdata.fitness[mdata_index] = fitness_content\n",
      "    #if re.search(\"\\s*[f|F]itness\\s*:\", summary4):\n",
      "    #    has_category = 1\n",
      "        # Extract Fitness Content\n",
      "    #    fitness_content = re.split('\\s*:\\s*', summary4)[1]\n",
      "    #    mdata.fitness[mdata_index] = fitness_content\n",
      "\n",
      "    # Extract Odor Content\n",
      "    #if re.search(\"\\s*[o|O]dor\\s*:\", summary1):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Odor Content\n",
      "    #    odor_content = re.split('\\s*:\\s*', summary1)[1]\n",
      "    #    mdata.odor[mdata_index] = odor_content\n",
      "    #if re.search(\"\\s*[o|O]dor\\s*:\", summary2):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Odor Content\n",
      "    #    odor_content = re.split('\\s*:\\s*', summary2)[1]\n",
      "    #    mdata.odor[mdata_index] = odor_content\n",
      "    #if re.search(\"\\s*[o|O]dor\\s*:\", summary3):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Odor Content\n",
      "    #    odor_content = re.split('\\s*:\\s*', summary3)[1]\n",
      "    #    mdata.odor[mdata_index] = odor_content\n",
      "    #if re.search(\"\\s*[o|O]dor\\s*:\", summary4):\n",
      "    #    has_category = 1\n",
      "    #    # Extract Odor Content\n",
      "    #    odor_content = re.split('\\s*:\\s*', summary4)[1]\n",
      "    #    mdata.odor[mdata_index] = odor_content\n",
      "\n",
      "    # Extract Headache Content\n",
      "    if re.search(\"[h|H]eadache:\", summary1):\n",
      "        has_category = 1\n",
      "        # Extract Headache Content\n",
      "        headache_content = re.split(': ', summary1)[1]\n",
      "        mdata.headache[mdata_index] = int(headache_content)\n",
      "    if re.search(\"[h|H]eadache:\", summary2):\n",
      "        has_category = 1\n",
      "        # Extract Headache Content\n",
      "        headache_content = re.split(': ', summary2)[1]\n",
      "        mdata.headache[mdata_index] = int(headache_content)\n",
      "    if re.search(\"[h|H]eadache:\", summary3):\n",
      "        has_category = 1\n",
      "        # Extract Headache Content\n",
      "        headache_content = re.split(': ', summary3)[1]\n",
      "        mdata.headache[mdata_index] = int(headache_content)\n",
      "    if re.search(\"[h|H]eadache:\", summary4):\n",
      "        has_category = 1\n",
      "        # Extract Headache Content\n",
      "        headache_content = re.split(': ', summary4)[1]\n",
      "        mdata.headache[mdata_index] = int(headache_content)\n",
      "\n",
      "    # Extract Glucose Content\n",
      "    if re.search(\"[g|G]lucose:\", summary1):\n",
      "        has_category = 1\n",
      "        # Extract Glucose Content\n",
      "        glucose_content = re.split(': ', summary1)[1]\n",
      "        mdata.glucose[mdata_index] = int(re.split(' ', glucose_content)[0])\n",
      "    if re.search(\"[g|G]lucose:\", summary2):\n",
      "        has_category = 1\n",
      "        # Extract Glucose Content\n",
      "        glucose_content = re.split(': ', summary2)[1]\n",
      "        mdata.glucose[mdata_index] = int(re.split(' ', glucose_content)[0])\n",
      "    if re.search(\"[g|G]lucose:\", summary3):\n",
      "        has_category = 1\n",
      "        # Extract Glucose Content\n",
      "        glucose_content = re.split(': ', summary3)[1]\n",
      "        mdata.glucose[mdata_index] = int(re.split(' ', glucose_content)[0])\n",
      "    if re.search(\"[g|G]lucose:\", summary4):\n",
      "        has_category = 1\n",
      "        # Extract Glucose Content\n",
      "        glucose_content = re.split(': ', summary4)[1]\n",
      "        mdata.glucose[mdata_index] = int(re.split(' ', glucose_content)[0])\n",
      "    \n",
      "    # Add generic event to Calendar_event1\n",
      "    # In next version, extend this to Calendar_eventX\n",
      "    if (has_category == 0):    \n",
      "        if (summary1 != \"nan\"):\n",
      "            mdata.calendar_event1[mdata_index] = summary1\n",
      "        elif (summary2 != \"nan\"):\n",
      "            mdata.calendar_event1[mdata_index] = summary2\n",
      "        elif (summary3 != \"nan\"):\n",
      "            mdata.calendar_event1[mdata_index] = summary3\n",
      "        elif (summary4 != \"nan\"):\n",
      "            mdata.calendar_event1[mdata_index] = summary4\n",
      "            \n",
      "elapsed = timeit.default_timer() - start_time\n",
      "print elapsed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "37586.1247248\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extend Headache measurements across time\n",
      "current_headache = 0\n",
      "counter_headache = 0\n",
      "current_eye_contacts = 0 #np.nan\n",
      "counter_eye_contacts = 0\n",
      "\n",
      "#for i in range(0, 400000):\n",
      "for i in range(0, 400000):\n",
      "    \n",
      "    mdata_index = mdata.index[i]\n",
      "    \n",
      "    if math.isnan(mdata.headache[mdata_index]):\n",
      "        if (counter_headache==1):\n",
      "            mdata.headache[mdata_index] = current_headache\n",
      "    else:\n",
      "        current_headache = mdata.headache[mdata_index]\n",
      "        counter_headache = 1\n",
      "        \n",
      "    if not isinstance(mdata.eye_contacts[mdata_index], basestring):\n",
      "    #if math.isnan(mdata.eye_contacts[mdata_index]):\n",
      "        if (counter_eye_contacts==1):\n",
      "            mdata.eye_contacts[mdata_index] = current_eye_contacts\n",
      "    else:\n",
      "        current_eye_contacts = mdata.eye_contacts[mdata_index]\n",
      "        counter_eye_contacts = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdata.to_csv('migraine_google_calendar_categorized.csv', index=True,header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Weather Data - This was not incorporated into project at this time. But we used weather-related api's to get weather data. This will be pursued later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Location = r'migraine_data.csv'\n",
      "frame = read_csv(Location)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#defining the columns for dataframe\n",
      "columns = ['latitude', 'longitude', 'location_name', 'last_updated', 'temp', 'text', 'uv_index', 'uv_text', 'visibility', 'humidity', 'barometer_direction', 'barometer_reading', 'feels_like', 'wind_gust', 'wind_direction', 'wind_speed', 'wind_text', 'dewpoint', 'moonphase_icon', 'moonphase_text']\n",
      "\n",
      "#setup empty dataframe with columns\n",
      "frame = DataFrame(columns=columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Access Weather Data via pywapi api"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#fetch data from www.weather.com\n",
      "#note, for this to work, i must be connected to the internet\n",
      "\n",
      "weather_com_result = pywapi.get_weather_from_weather_com('94578')\n",
      "#print weather_com_result\n",
      "#yahoo_result = pywapi.get_weather_from_yahoo('94578')\n",
      "#noaa_result = pywapi.get_weather_from_noaa('KJFK')\n",
      "\n",
      "latitude = weather_com_result['location']['lat']\n",
      "longitude = weather_com_result['location']['lon']\n",
      "location_name = weather_com_result['location']['name']\n",
      "last_updated = weather_com_result['current_conditions']['last_updated']\n",
      "temp = weather_com_result['current_conditions']['temperature']\n",
      "text = weather_com_result['current_conditions']['text']\n",
      "uv_index = weather_com_result['current_conditions']['uv']['index']\n",
      "uv_text = weather_com_result['current_conditions']['uv']['text']\n",
      "visibility = weather_com_result['current_conditions']['visibility']\n",
      "humidity = weather_com_result['current_conditions']['humidity']\n",
      "barometer_direction = weather_com_result['current_conditions']['barometer']['direction']\n",
      "barometer_reading = weather_com_result['current_conditions']['barometer']['reading']\n",
      "feels_like = weather_com_result['current_conditions']['feels_like']\n",
      "wind_gust = weather_com_result['current_conditions']['wind']['gust']\n",
      "wind_direction = weather_com_result['current_conditions']['wind']['direction']\n",
      "wind_speed = weather_com_result['current_conditions']['wind']['speed']\n",
      "wind_text = weather_com_result['current_conditions']['wind']['text']\n",
      "dewpoint = weather_com_result['current_conditions']['dewpoint']\n",
      "moonphase_icon = weather_com_result['current_conditions']['moon_phase']['icon'] \n",
      "moonphase_text = weather_com_result['current_conditions']['moon_phase']['text']\n",
      "\n",
      "print last_updated"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert last_updated into datetime element\n",
      "d1 = parser.parse(last_updated)\n",
      "print d1\n",
      "# Check that new update is not the same as the old\n",
      "#print (d1 != frame.irow(len(frame)-1)['date_time'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_weather_data():\n",
      "    data = [{'date_time': d1,\n",
      "        'last_updated': last_updated,\n",
      "        'temp': temp,\n",
      "        'text': text,\n",
      "        'uv_index': uv_index,\n",
      "        'uv_text': uv_text,\n",
      "        'visibility': visibility,\n",
      "        'humidity': humidity,\n",
      "        'barometer_direction': barometer_direction,\n",
      "        'barometer_reading': barometer_reading,\n",
      "        'feels_like': feels_like,\n",
      "        'wind_gust': wind_gust,\n",
      "        'wind_direction': wind_direction,\n",
      "        'wind_text': wind_text,\n",
      "        'latitude': latitude,\n",
      "        'longitude': longitude,\n",
      "        'location_name': location_name,\n",
      "        'moonphase_text': moonphase_text,\n",
      "        'moonphase_icon': moonphase_icon,\n",
      "        'dewpoint': dewpoint}]\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print get_weather_data()\n",
      "# YAY this works!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print \"this is original date \" + str(d1)\n",
      "def add_weather_to_frame(d1, data, frame):\n",
      "    for i in range(20):\n",
      "        d2 = d1 + timedelta(minutes=i)\n",
      "        #frame.append creates a new dataframe. save this dataframe to existing dataframe..\n",
      "        data[0]['date_time'] = d2\n",
      "        frame = frame.append(data, ignore_index=True)\n",
      "        return frame\n",
      "#    print d2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "my_data = get_weather_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print add_weather_to_frame(d1, my_data, frame)\n",
      "# YAY, it works!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frame.describe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if (d1 != frame.irow(len(frame)-1)['date_time']):\n",
      "#get_weather_data()\n",
      "print add_weather_to_frame(d1, my_data, frame)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print data[0]['date_time'] - this is how we index into the list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sanity check\n",
      "print frame\n",
      "#frame.to_csv('migraine_data.csv',index=False,header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Get Weather data with openweather 0.11 API"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import openweather\n",
      "from datetime import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create client\n",
      "ow = openweather.OpenWeather()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# find weather stations near me\n",
      "stations = ow.find_stations_near(\n",
      "    -122.1277,  # longitude\n",
      "    37.7053, # latitude\n",
      "    100   # kilometer radius\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "#Checking that order of stations is closest to farthest\n",
      "#print stations['list'][0]['distance']\n",
      "#print stations['list'][1]['distance']\n",
      "#print stations['list'][2]['distance']\n",
      "#print stations['list'][3]['distance']\n",
      "#print stations['list'][4]['distance']\n",
      "#print stations['list'][5]['distance']\n",
      "#print stations['list'][6]['distance']\n",
      "#print stations['list'][7]['distance']\n",
      "#print stations['list'][8]['distance']\n",
      "#print stations['list'][9]['distance']\n",
      "#print stations['list'][10]['distance']\n",
      "#print stations['list'][11]['distance']\n",
      "\n",
      "closest_station_id = stations['list'][0]['id']\n",
      "print closest_station_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# iterate results\n",
      "for station in stations:\n",
      "    print station"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ow.get_weather(closest_station_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# historic weather\n",
      "start_date = datetime(2014, 10, 10, 12, 12, 12)\n",
      "print start_date\n",
      "end_date   = datetime(2014, 10, 10, 12, 12, 12)\n",
      "print end_date\n",
      "#end_date   = datetime.strptime(\"2014-11-07 00:00:00\", '%Y-%m-%d %H:%M:%S') \n",
      "\n",
      "#end_date.second\n",
      "\n",
      "#print int(end_date.second())\n",
      "\n",
      "#from_ts = int(start_date.strftime('%s'))\n",
      "\n",
      "#delta = end_date - start_date\n",
      "#print int(delta.total_seconds())\n",
      "\n",
      "# default: hourly interval\n",
      "print ow.get_historic_weather(closest_station_id, start_date, end_date, \"hour\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# here, i need to check data from web. If it is new, add to dataframe....\n",
      "\n",
      "#from copy import deepcopy\n",
      "#data\n",
      "#new_data = deepcopy(data)\n",
      "#new_data = data[:]\n",
      "#data and new_data are a list of dicts!\n",
      "#new_data[0]['barometer_reading'] = 67\n",
      "#print data\n",
      "#print new_data\n",
      "#if (data == new_data):\n",
      "#    print \"they are the same\"\n",
      "#else:\n",
      "#    print \"they are not the same\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print \"Weather.com says: It is \" + string.lower(weather_com_result['current_conditions']['text']) + \" and \" + weather_com_result['current_conditions']['temperature'] + \"C now in San Leandro.\\n\\n\"\n",
      "\n",
      "# print \"Yahoo says: It is \" + string.lower(yahoo_result['condition']['text']) + \" and \" + yahoo_result['condition']['temp'] + \"C now in San Leandro.\\n\\n\"\n",
      "\n",
      "# print \"NOAA says: It is \" + string.lower(noaa_result['weather']) + \" and \" + noaa_result['temp_c'] + \"C now in San Leandro.\\n\"\n",
      "\n",
      "#print \"latitude is\", latitude\n",
      "#print \"longitude is\", longitude\n",
      "#print \"location is\", location_name \n",
      "#print \"last updated is \", last_updated\n",
      "#print \"temperature is\", temp\n",
      "#print \"text is \", text\n",
      "#print \"uv index is\", uv_index\n",
      "#print \"visibility is\", visibility\n",
      "#print \"humidity is\", humidity\n",
      "#print \"barometer direction is\", barometer_direction\n",
      "#print \"barometer reading is\", barometer_reading\n",
      "#print \"feels like is\", feels_like \n",
      "#print \"wind gust is\", wind_gust\n",
      "#print \"wind direction is\", wind_direction\n",
      "#print \"wind speed is\", wind_speed\n",
      "#print \"wind text is\", wind_text\n",
      "#print \"dewpoint is\", dewpoint\n",
      "#print \"moonphase icon is\", moonphase_icon, \" and moonphase text is\", moonphase_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Accessing Dropbox Files - I upload sensor and mobile data from Android phone to dropbox account. This data includes calendar, SMS log, call log, and GPS data. In future work, I expect to regularly access this data to populate streaming database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dropbox\n",
      "\n",
      "access_token = '8w_0NR1l6zAAAAAAAAABg5rGVF9L-tbnBAHOhGkjhVSuyTm2rVXj1lmZE3-V5Ee1'\n",
      "\n",
      "dropbox_client = dropbox.client.DropboxClient(access_token)\n",
      "print 'linked account: ', dropbox_client.account_info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Access contents of GPS Logger Directory\n",
      "folder_metadata = dropbox_client.metadata('/Apps/GPSLogger for Android')\n",
      "#print \"metadata:\", folder_metadata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#f, metadata = dropbox_client.get_file_and_metadata('/Apps/GPSLogger for Android/20140401.txt')\n",
      "#out = open('/Apps/GPSLogger for Android/20131122.txt', 'wb')\n",
      "#out.write(f.read())\n",
      "#out.close()\n",
      "#print metadata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?dropbox_client.get_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table = pd.read_table('tmp.sv', sep='|')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print basis_user_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ics import Calendar, Event"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pybasis API - provides convenient way to access Basis data. in future versions, I will clean and incorporate sleep data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pybasis\n",
      "\n",
      "# basisAPI = pybasis.basisAPI(\"\",\"\")\n",
      "\n",
      "# basisAPI.sleepData('2014-02-12')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_metrics = basisAPI.getPhysMetrics('2014-03-07', '2014-12-01')\n",
      "#full_metrics.viewkeys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print full_metrics['endtime']\n",
      "print full_metrics['timezone_history']\n",
      "print full_metrics['starttime']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1417517940\n",
        "[{u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1394168400}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1394254800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394359200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394341200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394359200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394424000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394510400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394596800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394683200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394769600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394856000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1394942400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395028800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395115200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395201600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395288000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395374400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395460800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395547200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395633600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395720000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395806400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395892800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1395979200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396065600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396152000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396238400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396324800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396411200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396497600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396584000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396670400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396756800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396843200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1396929600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397016000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397102400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397188800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397275200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397361600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397448000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397534400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397620800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397707200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397793600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397880000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1397966400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398052800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398139200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398225600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398312000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398398400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398484800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398571200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398657600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398744000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398830400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1398916800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399003200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399089600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399176000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399262400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399348800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399435200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399521600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399608000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399694400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399780800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399867200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1399953600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400040000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400126400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400212800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400299200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400385600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400472000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400558400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400644800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400731200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400817600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400904000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1400990400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401076800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401163200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401249600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401336000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401422400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401508800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401595200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401681600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401768000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401854400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1401940800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402027200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402113600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402200000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402286400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402372800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402459200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402545600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402632000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402718400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402804800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402891200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1402977600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403064000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403150400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403236800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403323200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403409600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403496000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403582400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403668800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403755200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403841600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1403928000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404014400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404100800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404187200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404273600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404360000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404446400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404532800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404619200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404705600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404792000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404878400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1404964800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405051200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405137600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405224000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405310400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405396800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405483200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405569600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405656000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405742400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405828800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1405915200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406001600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406088000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406174400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406260800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406347200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406433600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406520000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406606400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406692800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406779200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406865600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1406952000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407038400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407124800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407211200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407297600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407384000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407470400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407556800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407643200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407729600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407816000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407902400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1407988800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408075200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408161600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408248000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408334400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408420800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408507200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408593600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408680000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408766400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408852800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1408939200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409025600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409112000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409198400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409284800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409371200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409457600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409544000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409630400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409716800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409803200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409889600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1409976000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410062400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410148800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410235200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410321600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410408000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410494400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410580800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410667200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410753600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410840000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1410926400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411012800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411099200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411185600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411272000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411358400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411444800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411531200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411617600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411704000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411790400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411876800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1411963200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412049600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412136000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412222400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412308800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412395200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412481600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412568000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412654400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412740800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412827200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1412913600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413000000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413086400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413172800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413259200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413345600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413432000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413518400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413604800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413691200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413777600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413864000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1413950400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414036800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414123200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414209600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414296000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414382400}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414468800}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414555200}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414641600}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414728000}, {u'timezone': u'America/Los_Angeles', u'offset': -7.0, u'start': 1414814400}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1414918800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1414900800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1414918800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1414990800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415077200}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415163600}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415250000}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415336400}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415422800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415509200}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415595600}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415682000}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415768400}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415854800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1415941200}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416027600}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416114000}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416200400}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416286800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416373200}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416459600}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416546000}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416632400}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416718800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416805200}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416891600}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1416978000}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1417064400}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1417150800}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1417237200}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1417323600}, {u'timezone': u'America/Los_Angeles', u'offset': -8.0, u'start': 1417410000}]\n",
        "1394168400\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get timezone offset from Basis\n",
      "offset_num = int(full_metrics['timezone_history'][0]['offset'] * -1)\n",
      "\n",
      "# Convert Basis starttime to string\n",
      "b_starttime_index_str =  str(pd.datetime.fromtimestamp(full_metrics['starttime'])) + \"-0\" + str(offset_num) + \":00\"\n",
      "b_endtime_index_str   =  str(pd.datetime.fromtimestamp(full_metrics['endtime']))   + \"-0\" + str(offset_num) + \":00\"\n",
      "\n",
      "# Convert Basis starttime string to datetime object\n",
      "#b_starttime_index = pd.to_datetime(b_starttime_index_str)\n",
      "#b_endtime_index = pd.to_datetime(b_endtime_index_str)\n",
      "\n",
      "\n",
      "b_starttime_index =  pd.Timestamp(b_starttime_index_str, tz='US/Pacific')\n",
      "b_endtime_index   =  pd.Timestamp(b_endtime_index_str, tz='US/Pacific')\n",
      "\n",
      "print b_starttime_index\n",
      "print b_endtime_index\n",
      "\n",
      "time_duration = b_endtime_index - b_starttime_index\n",
      "print time_duration\n",
      "# I used .days and .seconds because .min did not work.\n",
      "# Added 1 to delta_min\n",
      "delta_min = (time_duration.days * 1440) + (time_duration.seconds / 60) + 1\n",
      "#print delta_min\n",
      "print delta_min"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-03-06 21:00:00-08:00\n",
        "2014-12-02 02:59:00-08:00\n",
        "270 days, 5:59:00\n",
        "389160\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add calendar event descriptions to mdata dataframe\n",
      "for i in range(int(delta_min)):\n",
      "#for i in range(400000):\n",
      "    mdata.air_temp[b_starttime_index + timedelta(minutes=i)]  = full_metrics['metrics']['air_temp'][i]\n",
      "    mdata.gsr[b_starttime_index + timedelta(minutes=i)]       = full_metrics['metrics']['gsr'][i]\n",
      "    mdata.heartrate[b_starttime_index + timedelta(minutes=i)] = full_metrics['metrics']['heartrate'][i]\n",
      "    mdata.calories[b_starttime_index + timedelta(minutes=i)]  = full_metrics['metrics']['calories'][i]\n",
      "    mdata.skin_temp[b_starttime_index + timedelta(minutes=i)] = full_metrics['metrics']['skin_temp'][i]\n",
      "    mdata.steps[b_starttime_index + timedelta(minutes=i)]     = full_metrics['metrics']['steps'][i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save dataframe to CSV and check results\n",
      "mdata.to_csv('migraine_google_basis.csv', index=True,header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sleepData = basisAPI.sleepData('2014-03-08')\n",
      "#print sleepData\n",
      "print len(sleepData) # Number of times I went to sleep\n",
      "\n",
      "# import csv\n",
      "# csv.DictReader"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'content'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-102-c26e8ffff117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msleepData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbasisAPI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleepData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2014-03-08'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print sleepData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleepData\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Number of times I went to sleep\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pybasis\\client.pyc\u001b[0m in \u001b[0;36msleepData\u001b[1;34m(self, startdate, enddate)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://app.mybasis.com/api/v2/users/me/days/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstartdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'YYYY-MM-DD'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/activities?type=sleep&expand=activities.stages,activities.events\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0msleepList\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'activities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msleepList\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: 'content'"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sleepData[0]['sleep']['quality']\n",
      "print sleepData[0]['end_time']['iso']\n",
      "print sleepData[0]['end_time']['time_zone']\n",
      "print sleepData[0]['end_time']['timestamp']\n",
      "\n",
      "print sleepData[1]['sleep']['quality']\n",
      "print sleepData[1]['end_time']['iso']\n",
      "print sleepData[1]['end_time']['time_zone']\n",
      "print sleepData[1]['end_time']['timestamp']\n",
      "\n",
      "# Sleep quality is in effect AFTER last sleep time!!!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "96\n",
        "2014-10-08T19:17:00Z\n",
        "{u'name': u'America/Los_Angeles', u'offset': -420}\n",
        "1412795820\n",
        "97\n",
        "2014-10-09T02:16:00Z\n",
        "{u'name': u'America/Los_Angeles', u'offset': -420}\n",
        "1412820960\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sleepData['end_time']\n",
      "print sleepData['timezone_history']\n",
      "print sleepData['start_time']\n",
      "print sleepData['sleep']['quality']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "list indices must be integers, not str",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-59-fcc7aab6db8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0msleepData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end_time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0msleepData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timezone_history'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0msleepData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0msleepData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sleep'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'quality'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: list indices must be integers, not str"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How to locate a specific row in a dataframe...\n",
      "# mdata.loc['2014-10-22 18:18:00']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "In the future, incorporate Factual API - can provide ingredients for all foods and medicines consumed."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}